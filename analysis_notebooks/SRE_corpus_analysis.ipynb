{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRE Corpus Result Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ot\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import matplotlib.patheffects as path_effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Data From Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prior_rating_trials = pd.read_csv(\"../data/qof-ratings/human_ratings/1215DenseRatingCheck.csv\")\n",
    "remnant_rating_trials = pd.read_csv(\"../data/qof-ratings/human_ratings/1215DenseRatingRemnant.csv\")\n",
    "remnant_rating_trials_more = pd.read_csv(\"../data/qof-ratings/human_ratings/1231DenseRatingRemnant.csv\")\n",
    "human_rating_trials = pd.concat([human_prior_rating_trials, remnant_rating_trials, remnant_rating_trials_more]).reset_index()\n",
    "human_rating_responses = human_rating_trials[\"answer\"].dropna().apply(json.loads).apply(pd.Series)\n",
    "human_ratings = human_rating_responses[\"answer\"].apply(pd.Series)\n",
    "human_ratings_full = pd.merge(\n",
    "    human_rating_responses.drop(columns=[\"tones\", \"answer\"]),\n",
    "    human_ratings,\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ").groupby(\"sentence\").agg(\n",
    "    lambda x: np.mean(x.dropna().astype(float))\n",
    ")\n",
    "human_ratings_full = human_ratings_full.reindex(sorted(human_ratings_full.columns), axis=1)\n",
    "human_ratings_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_rating_trials = pd.read_csv(f\"../data/qof-ratings/gpt_ratings.csv\", sep=\"|\")\n",
    "gpt_ratings_full = pd.pivot_table(\n",
    "    gpt_rating_trials,\n",
    "    index=\"sentence\",\n",
    "    columns=\"tone\",\n",
    "    values=\"current_rating\"\n",
    ")\n",
    "human_ratings_full = human_ratings_full.loc[gpt_ratings_full.index.str.replace(\"'\", \"`\"), :]\n",
    "gpt_ratings_full.index = gpt_ratings_full.index.str.replace(\"'\", \"`\")\n",
    "gpt_ratings_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not (human_ratings_full.values==0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not (gpt_ratings_full.values==0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (gpt_ratings_full.index == human_ratings_full.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (gpt_ratings_full.columns == human_ratings_full.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_sorting_label = dendrogram(linkage(human_ratings_full.values.T))[\"leaves\"]\n",
    "gpt_sorting_label = human_sorting_label # dendrogram(linkage(gpt_ratings_full.values.T))[\"leaves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 20))\n",
    "sns.heatmap(\n",
    "    human_ratings_full.corr().iloc[human_sorting_label, human_sorting_label],\n",
    "    vmin=-1, vmax=1,\n",
    "    cmap=\"coolwarm\"\n",
    ")\n",
    "plt.title(\"Human Conversation Tone Correlation Matrix\", fontsize=30)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=18)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 20))\n",
    "sns.heatmap(\n",
    "    gpt_ratings_full.corr().iloc[gpt_sorting_label, gpt_sorting_label],\n",
    "    vmin=-1, vmax=1,\n",
    "    cmap=\"coolwarm\"\n",
    ")\n",
    "\n",
    "plt.title(\"GPT Conversation Tone Correlation Matrix\", fontsize=30)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=18)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_ratings_full_copy = human_ratings_full.copy()\n",
    "human_ratings_full_copy.columns = [f\"Human: {x}\" for x in human_ratings_full_copy.columns]\n",
    "\n",
    "gpt_ratings_full_copy = gpt_ratings_full.copy()\n",
    "gpt_ratings_full_copy.columns = [f\"GPT: {x}\" for x in gpt_ratings_full_copy.columns]\n",
    "gpt_ratings_full_copy.index = gpt_ratings_full_copy.index.str.replace(\"'\", \"`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50, 40))\n",
    "cross_corr_matrix = pd.concat(\n",
    "    [\n",
    "        human_ratings_full_copy.T.iloc[human_sorting_label],\n",
    "        gpt_ratings_full_copy.T.iloc[gpt_sorting_label]\n",
    "    ]\n",
    ").T.corr()\n",
    "sns.heatmap(\n",
    "    cross_corr_matrix.iloc[:40, 40:]\n",
    ")\n",
    "ax.tick_params(axis='both', which='major', labelsize=30)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=30)\n",
    "cbar = ax.collections[0].colorbar\n",
    "# here set the labelsize by 20\n",
    "cbar.ax.tick_params(labelsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50, 40))\n",
    "sns.heatmap(\n",
    "    cross_corr_matrix\n",
    ")\n",
    "ax.tick_params(axis='both', which='major', labelsize=22)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=22)\n",
    "cbar = ax.collections[0].colorbar\n",
    "# here set the labelsize by 20\n",
    "cbar.ax.tick_params(labelsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Correlation MDS Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining MDS Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_corr_matrix_MDS = MDS(random_state=42).fit_transform(cross_corr_matrix.values)\n",
    "indices = cross_corr_matrix.index.str.split(\": \")\n",
    "MDS_with_source_tones = pd.DataFrame(cross_corr_matrix_MDS)\\\n",
    "    .assign(source=[elem[0] for elem in indices], tone=[elem[1] for elem in indices])\n",
    "MDS_with_source_tones = MDS_with_source_tones.rename(columns={0: \"MDS_x\", 1: \"MDS_y\"})\n",
    "same_tone_indices = pd.merge(\n",
    "    MDS_with_source_tones.query(\"source=='Human'\"),\n",
    "    MDS_with_source_tones.query(\"source=='GPT'\"),\n",
    "    left_on=\"tone\", right_on=\"tone\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_corr_matrix_as_coords = cross_corr_matrix.reset_index()\n",
    "cross_corr_matrix_as_coords.columns = range(cross_corr_matrix_as_coords.columns.size)\n",
    "cross_corr_matrix_as_coords[\"source\"] = cross_corr_matrix_as_coords[0].apply(lambda x: x.split(\": \")[0])\n",
    "cross_corr_matrix_as_coords[\"tone\"] = cross_corr_matrix_as_coords[0].apply(lambda x: x.split(\": \")[1])\n",
    "cross_corr_matrix_as_coords = cross_corr_matrix_as_coords.drop(columns=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Feature Ratings and Arrowmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.cm import rainbow\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tone_features = pd.read_csv(\"../data/tone-feature-ratings/GPT-tones-features.csv\", sep=\"|\")\n",
    "gpt_tone_features = gpt_tone_features.groupby([\"feature\", \"tone\"]).mean()[\"current_rating\"].reset_index()\n",
    "human_tone_features = pd.DataFrame(\n",
    "    list(pd.read_csv(\"../data/tone-feature-ratings/human-tones-features.csv\")[\"answer\"]\\\n",
    "        .dropna()\\\n",
    "        .apply(json.loads))\n",
    ").dropna()\n",
    "human_tone_features[\"answer\"] = human_tone_features[\"answer\"].apply(lambda x: int(list(x.values())[0]))\n",
    "human_tone_features = human_tone_features.groupby([\"feature\", \"tones\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MDS_biplot_dir(biplot_direction_side, feature_df):\n",
    "    # def get_MDS_biplot_directions(MDS_coord, tone_index_to_sort_by):\n",
    "    feature_df = feature_df[feature_df[\"tones\"].isin(same_tone_indices[\"tone\"])]\n",
    "    cc_MDS_sided = MDS_with_source_tones.query(f\"source=='{biplot_direction_side}'\")\n",
    "    cc_MDS_sided = cc_MDS_sided[cc_MDS_sided[\"tone\"].isin(same_tone_indices[\"tone\"])]\n",
    "    tone_index_to_sort_by = cc_MDS_sided[\"tone\"]\n",
    "    feature_coeff_dict = {}\n",
    "    feature_raw_dict = {}\n",
    "    for feature in feature_df[\"feature\"].unique():\n",
    "        cc_MDS_sided_feated = feature_df\\\n",
    "            .query(f\"feature=='{feature}'\")\\\n",
    "            .set_index(\"tones\")\\\n",
    "            .loc[tone_index_to_sort_by][\"answer\"]\n",
    "        linreg = LinearRegression().fit(\n",
    "            cc_MDS_sided[[\"MDS_x\", \"MDS_y\"]].values,\n",
    "            # cc_MDS_sided_feated\n",
    "            (cc_MDS_sided_feated - np.mean(cc_MDS_sided_feated)) / np.std(cc_MDS_sided_feated)\n",
    "        )\n",
    "        feature_coeff_dict[feature] = linreg.coef_\n",
    "        feature_raw_dict[feature] = cc_MDS_sided_feated\n",
    "    return feature_coeff_dict, feature_raw_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_biplot_dir, gpt_feature_dir = get_MDS_biplot_dir(\n",
    "    \"GPT\",\n",
    "    gpt_tone_features.rename(\n",
    "        columns={\n",
    "            \"current_rating\": \"answer\",\n",
    "            \"tone\": \"tones\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "gpt_biplot_dir = {\n",
    "    k: gpt_biplot_dir[k] for k in [\"arousal\", \"Informational\", \"positive in valence\", \"Relational\"]\n",
    "}\n",
    "gpt_feature_dir = {\n",
    "    k: gpt_feature_dir[k] for k in [\"arousal\", \"Informational\", \"positive in valence\", \"Relational\"]\n",
    "}\n",
    "human_biplot_dir, human_feature_dir = get_MDS_biplot_dir(\n",
    "    \"Human\",\n",
    "    human_tone_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Explained Variance for Arrowmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_feature_vec(feature, ccm_MDS, biplot_direction_side, feature_df, target_ccm_MDS=None):\n",
    "    indices = cross_corr_matrix.index.str.split(\": \")\n",
    "    MDS_with_source_tones = pd.DataFrame(ccm_MDS)\\\n",
    "        .assign(source=[elem[0] for elem in indices], tone=[elem[1] for elem in indices])\n",
    "    MDS_with_source_tones = MDS_with_source_tones.rename(columns={0: \"MDS_x\", 1: \"MDS_y\"})\n",
    "\n",
    "    feature_df = feature_df[feature_df[\"tones\"].isin(same_tone_indices[\"tone\"])]\n",
    "    cc_MDS_sided = MDS_with_source_tones.query(f\"source=='{biplot_direction_side}'\")\n",
    "    cc_MDS_sided = cc_MDS_sided[cc_MDS_sided[\"tone\"].isin(same_tone_indices[\"tone\"])]\n",
    "    tone_index_to_sort_by = cc_MDS_sided[\"tone\"]\n",
    "    \n",
    "    cc_MDS_sided_feated = feature_df\\\n",
    "        .query(f\"feature=='{feature}'\")\\\n",
    "        .set_index(\"tones\")\\\n",
    "        .loc[tone_index_to_sort_by][\"answer\"]\n",
    "    \n",
    "    linreg = LinearRegression().fit(\n",
    "        cc_MDS_sided[[\"MDS_x\", \"MDS_y\"]].values,\n",
    "        # cc_MDS_sided_feated\n",
    "        (cc_MDS_sided_feated - np.mean(cc_MDS_sided_feated)) / np.std(cc_MDS_sided_feated)\n",
    "    )\n",
    "    f_vec = linreg.coef_\n",
    "    return f_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_feature_variance(feature, ccm_MDS, biplot_direction_side, feature_df, target_ccm_MDS=None):\n",
    "    f_vec = get_one_feature_vec(feature, ccm_MDS, biplot_direction_side, feature_df, target_ccm_MDS=None)\n",
    "    \n",
    "    if target_ccm_MDS is None:\n",
    "        target_ccm_MDS = ccm_MDS\n",
    "    \n",
    "    normed_f_vec = f_vec / np.linalg.norm(f_vec)\n",
    "    mds_cov = np.cov(target_ccm_MDS.T)\n",
    "    return (\n",
    "        normed_f_vec.reshape((1, 2)) @ mds_cov @ normed_f_vec.reshape((2, 1)) / np.trace(mds_cov)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_exp_vars = {}\n",
    "\n",
    "for gpt_features in gpt_biplot_dir:\n",
    "    gpt_exp_vars[gpt_features] = get_one_feature_variance(\n",
    "        gpt_features, cross_corr_matrix_MDS, \"GPT\", gpt_tone_features.rename(\n",
    "            columns={\n",
    "                \"current_rating\": \"answer\",\n",
    "                \"tone\": \"tones\"\n",
    "            }\n",
    "        )\n",
    "    )[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_exp_vars = {}\n",
    "\n",
    "for human_features in human_biplot_dir:\n",
    "    human_exp_vars[human_features] = get_one_feature_variance(\n",
    "        human_features, cross_corr_matrix_MDS, \"GPT\", human_tone_features.rename(\n",
    "            columns={\n",
    "                \"current_rating\": \"answer\",\n",
    "                \"tone\": \"tones\"\n",
    "            }\n",
    "        )\n",
    "    )[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Correlation MDS-Biplot with Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "projection = lambda pt1, pt2: np.dot(pt1, pt2) / np.dot(pt2, pt2)\n",
    "for pt, word in zip(cross_corr_matrix_MDS, cross_corr_matrix.index):\n",
    "    if \"Human\" in word:\n",
    "        projected_score_on_valence = 1 - cosine(pt, human_biplot_dir[\"positive in valence\"])\n",
    "        projected_score_on_arousal = 1 - cosine(pt, human_biplot_dir[\"aroused\"])\n",
    "        color = \"red\"\n",
    "    elif \"GPT\" in word:\n",
    "        projected_score_on_valence = 1 - cosine(pt, gpt_biplot_dir[\"positive in valence\"])\n",
    "        projected_score_on_arousal = 1 - cosine(pt, gpt_biplot_dir[\"aroused\"])\n",
    "        color = \"blue\"\n",
    "    plt.scatter(\n",
    "        x=pt[0], y=pt[1], c=color  # [(projected_score_on_valence / 2 + 0.5, 0, projected_score_on_arousal / 2 + 0.5)]\n",
    "    )\n",
    "    ax.annotate(word, [pt[0] - 0.1, pt[1] + 0.05], ha=\"center\")\n",
    "\n",
    "for row_id in same_tone_indices.index:\n",
    "    gpt_aligned_pt = [same_tone_indices.iloc[row_id, 4], same_tone_indices.iloc[row_id, 5]]\n",
    "    human_pt = [same_tone_indices.iloc[row_id, 0], same_tone_indices.iloc[row_id, 1]]\n",
    "    plt.plot(\n",
    "        [gpt_aligned_pt[0], human_pt[0]],\n",
    "        [gpt_aligned_pt[1], human_pt[1]],\n",
    "        # color = \"red\",\n",
    "        # alpha = (1 - euclidean(translate_aligned_pt, rewrite_pt) / 5) ** 3\n",
    "        path_effects=[path_effects.SimpleLineShadow(offset=(0, 0), shadow_color='black', alpha=0.3, linewidth=12)]\n",
    "    )\n",
    "\n",
    "for feature in gpt_biplot_dir:\n",
    "    coord_x = gpt_biplot_dir[feature][0] * 4\n",
    "    coord_y = gpt_biplot_dir[feature][1] * 4\n",
    "    plt.arrow(0, 0, coord_x, coord_y, head_width=0.02, color=\"blue\", width=0.01)\n",
    "    ax.annotate(\n",
    "        feature, [coord_x, coord_y], [coord_x * 1.1 + int(coord_x > 0) * 0.15, coord_y * 1.1], color=\"blue\", ha=\"center\"\n",
    "    )\n",
    "\n",
    "for feature in human_biplot_dir:\n",
    "    coord_x = human_biplot_dir[feature][0] * 4\n",
    "    coord_y = human_biplot_dir[feature][1] * 4\n",
    "    plt.arrow(0, 0, coord_x, coord_y, head_width=0.02, color=\"red\", width=0.01)\n",
    "    ax.annotate(\n",
    "        feature, [coord_x, coord_y], [coord_x * 1.1 + 0.05, coord_y * 1.1 - 0.03], color=\"red\", ha=\"center\"\n",
    "    )\n",
    "\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "projection = lambda pt1, pt2: np.dot(pt1, pt2) / np.dot(pt2, pt2)\n",
    "for pt, word in zip(cross_corr_matrix_MDS, cross_corr_matrix.index):\n",
    "    if \"Human\" in word:\n",
    "        projected_score_on_valence = 1 - cosine(pt, human_biplot_dir[\"positive in valence\"])\n",
    "        projected_score_on_arousal = 1 - cosine(pt, human_biplot_dir[\"aroused\"])\n",
    "        color = \"red\"\n",
    "    elif \"GPT\" in word:\n",
    "        projected_score_on_valence = 1 - cosine(pt, gpt_biplot_dir[\"positive in valence\"])\n",
    "        projected_score_on_arousal = 1 - cosine(pt, gpt_biplot_dir[\"aroused\"])\n",
    "        color = \"blue\"\n",
    "    plt.scatter(\n",
    "        x=pt[0], y=pt[1], c=color  # [(projected_score_on_valence / 2 + 0.5, 0, projected_score_on_arousal / 2 + 0.5)]\n",
    "    )\n",
    "    # ax.annotate(word, [pt[0] - 0.1, pt[1] + 0.05], ha=\"center\")\n",
    "\n",
    "for row_id in same_tone_indices.index:\n",
    "    gpt_aligned_pt = [same_tone_indices.iloc[row_id, 4], same_tone_indices.iloc[row_id, 5]]\n",
    "    human_pt = [same_tone_indices.iloc[row_id, 0], same_tone_indices.iloc[row_id, 1]]\n",
    "    plt.plot(\n",
    "        [gpt_aligned_pt[0], human_pt[0]],\n",
    "        [gpt_aligned_pt[1], human_pt[1]],\n",
    "        # color = \"red\",\n",
    "        # alpha = (1 - euclidean(translate_aligned_pt, rewrite_pt) / 5) ** 3\n",
    "        path_effects=[path_effects.SimpleLineShadow(offset=(0, 0), shadow_color='black', alpha=0.3, linewidth=12)]\n",
    "    )\n",
    "\n",
    "for feature in gpt_biplot_dir:\n",
    "    coord_x = gpt_biplot_dir[feature][0] * 4\n",
    "    coord_y = gpt_biplot_dir[feature][1] * 4\n",
    "    plt.arrow(0, 0, coord_x, coord_y, head_width=0.02, color=\"blue\", width=0.01)\n",
    "    # ax.annotate(\n",
    "    #     feature, [coord_x, coord_y], [coord_x * 1.1 + int(coord_x > 0) * 0.15, coord_y * 1.1], color=\"blue\", ha=\"center\"\n",
    "    # )\n",
    "\n",
    "for feature in human_biplot_dir:\n",
    "    coord_x = human_biplot_dir[feature][0] * 4\n",
    "    coord_y = human_biplot_dir[feature][1] * 4\n",
    "    plt.arrow(0, 0, coord_x, coord_y, head_width=0.02, color=\"red\", width=0.01)\n",
    "    # ax.annotate(\n",
    "    #     feature, [coord_x, coord_y], [coord_x * 1.1 + 0.05, coord_y * 1.1 - 0.03], color=\"red\", ha=\"center\"\n",
    "    # )\n",
    "\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Analyses for Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_feature_shared_explained_var = {}\n",
    "for MDS_seed in range(5000):\n",
    "    cur_ccm_mds = MDS(random_state=MDS_seed).fit_transform(cross_corr_matrix.values)\n",
    "    for gpt_features in gpt_biplot_dir:\n",
    "        if gpt_features not in gpt_feature_shared_explained_var:\n",
    "            gpt_feature_shared_explained_var[gpt_features] = []\n",
    "        gpt_feature_shared_explained_var[gpt_features].append(\n",
    "            get_one_feature_variance(gpt_features, cur_ccm_mds, \"GPT\", gpt_tone_features.rename(\n",
    "        columns={\n",
    "            \"current_rating\": \"answer\",\n",
    "            \"tone\": \"tones\"\n",
    "        }\n",
    "    ))[0][0]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpt_features in gpt_feature_shared_explained_var:\n",
    "    print(\n",
    "        (\n",
    "            gpt_features,\n",
    "            np.mean(gpt_feature_shared_explained_var[gpt_features]),\n",
    "            np.std(gpt_feature_shared_explained_var[gpt_features]),\n",
    "            (\n",
    "                np.percentile(gpt_feature_shared_explained_var[gpt_features], 2.5),\n",
    "                np.percentile(gpt_feature_shared_explained_var[gpt_features], 97.5)\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_feature_human_space_explained_var = {}\n",
    "for MDS_seed in range(5000):\n",
    "    cur_ccm_mds = MDS(random_state=MDS_seed).fit_transform(cross_corr_matrix.values)\n",
    "    for human_features in human_biplot_dir:\n",
    "        if human_features not in human_feature_human_space_explained_var:\n",
    "            human_feature_human_space_explained_var[human_features] = []\n",
    "        human_feature_human_space_explained_var[human_features].append(\n",
    "            get_one_feature_variance(human_features, cur_ccm_mds, \"Human\", human_tone_features, cur_ccm_mds[:40])[0][0]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for human_features in human_feature_human_space_explained_var:\n",
    "    print(\n",
    "        (\n",
    "            human_features,\n",
    "            np.mean(human_feature_human_space_explained_var[human_features]),\n",
    "            np.std(human_feature_human_space_explained_var[human_features]),\n",
    "            (\n",
    "                np.percentile(human_feature_human_space_explained_var[human_features], 2.5),\n",
    "                np.percentile(human_feature_human_space_explained_var[human_features], 97.5)\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle_between_vecs(v1, v2):\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return (np.dot(v1 / norm_v1, v2 / norm_v2))\n",
    "\n",
    "cosines_bootstrap_stats = {human_features: [] for human_features in human_biplot_dir}\n",
    "for MDS_seed in range(5000):\n",
    "    cur_ccm_mds = MDS(random_state=MDS_seed).fit_transform(cross_corr_matrix.values)\n",
    "    for human_features, gpt_features in zip(human_biplot_dir, gpt_biplot_dir):\n",
    "        human_angle = get_one_feature_vec(human_features, cur_ccm_mds, \"Human\", human_tone_features, cur_ccm_mds[:40])\n",
    "        gpt_angle = get_one_feature_vec(\n",
    "            gpt_features, cur_ccm_mds, \"GPT\",\n",
    "            gpt_tone_features.rename(\n",
    "                columns={\n",
    "                    \"current_rating\": \"answer\",\n",
    "                    \"tone\": \"tones\"\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        cosines_bootstrap_stats[human_features].append(get_angle_between_vecs(human_angle, gpt_angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for human_features in human_biplot_dir:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "            {human_features}\n",
    "            mean: {np.mean(cosines_bootstrap_stats[human_features])}\n",
    "            lower_ci: {np.percentile(cosines_bootstrap_stats[human_features], 2.5)}\n",
    "            higher_ci: {np.percentile(cosines_bootstrap_stats[human_features], 97.5)}\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix Splithalf Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halfsplit_correlation(df):\n",
    "    df_shuffled = df.sample(frac=1.0)\n",
    "    halfsplit_a, halfsplit_b = df_shuffled[:len(df_shuffled) // 2], df_shuffled[len(df_shuffled) // 2:]\n",
    "    halfsplit_a_corr_triu = halfsplit_a.corr().values[np.triu_indices(40, 1)]\n",
    "    halfsplit_b_corr_triu = halfsplit_b.corr().values[np.triu_indices(40, 1)]\n",
    "    return np.corrcoef(halfsplit_a_corr_triu, halfsplit_b_corr_triu)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_species_corr(df1, df2):\n",
    "    halfsplit_a, halfsplit_b = df1.sample(frac=1.0, replace=True), df2.sample(frac=1.0, replace=True)\n",
    "    halfsplit_a_corr_triu = halfsplit_a.corr().values[np.triu_indices(40, 1)]\n",
    "    halfsplit_b_corr_triu = halfsplit_b.corr().values[np.triu_indices(40, 1)]\n",
    "    return np.corrcoef(halfsplit_a_corr_triu, halfsplit_b_corr_triu)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_species_halfsplit_corr = []\n",
    "while len(cross_species_halfsplit_corr) <= 5000:\n",
    "    corr_value = get_cross_species_corr(human_ratings_full, gpt_ratings_full)\n",
    "    if not np.isnan(corr_value):\n",
    "        cross_species_halfsplit_corr.append(corr_value)\n",
    "plt.hist(cross_species_halfsplit_corr)\n",
    "np.mean(cross_species_halfsplit_corr), np.percentile(cross_species_halfsplit_corr, 2.5), np.percentile(cross_species_halfsplit_corr, 97.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_halfsplit_corr = []\n",
    "for _ in range(5000):\n",
    "    human_halfsplit_corr.append(get_halfsplit_correlation(human_ratings_full))\n",
    "plt.hist(human_halfsplit_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_halfsplit_corr = []\n",
    "for _ in range(5000):\n",
    "    appended = get_halfsplit_correlation(gpt_ratings_full)\n",
    "    if np.isnan(appended):\n",
    "        continue\n",
    "    gpt_halfsplit_corr.append(appended)\n",
    "plt.hist(gpt_halfsplit_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_gpt_halfsplit_corr = []\n",
    "for _ in range(5000):\n",
    "    human_gpt_halfsplit_corr.append(\n",
    "        get_halfsplit_correlation(\n",
    "            pd.concat(\n",
    "                [\n",
    "                    human_ratings_full_copy.T,\n",
    "                    gpt_ratings_full_copy.T\n",
    "                ]\n",
    "            ).T\n",
    "        )\n",
    "    )\n",
    "plt.hist(human_gpt_halfsplit_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Correlation Diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.heatmap(cross_corr_matrix.iloc[:40, 40:],\n",
    "    vmin=-1, vmax=1,\n",
    "    cmap = LinearSegmentedColormap.from_list('my_gradient', (\n",
    "    # Edit this gradient at https://eltos.github.io/gradient/#00876C-FFFAA8-D43D51\n",
    "    (0.000, (0.000, 0.529, 0.424)),\n",
    "    (0.500, (1.000, 0.980, 0.659)),\n",
    "    (1.000, (0.831, 0.239, 0.318)))))\n",
    "plt.title(\"Conversation Tones Cross-Correlation Matrix\", fontsize=16)\n",
    "ax.set_xticklabels([s._text.split(\": \")[1] for s in ax.get_xticklabels()], fontsize=12)\n",
    "ax.set_yticklabels([s._text.split(\": \")[1] for s in ax.get_yticklabels()], fontsize=12)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "cross_corr_quadrant = cross_corr_matrix.iloc[40:, :40].stack().reset_index()\\\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"level_0\": \"gpt_tones\",\n",
    "            \"level_1\": \"human_tones\",\n",
    "            0: \"correlation\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "cross_corr_quadrant[[\"_1\", \"GPT_tone_name\"]] = cross_corr_quadrant[\"gpt_tones\"].str.split(\": \", expand=True)\n",
    "cross_corr_quadrant[[\"_1\", \"human_tone_name\"]] = cross_corr_quadrant[\"human_tones\"].str.split(\": \", expand=True)\n",
    "cross_corr_quadrant = cross_corr_quadrant.drop(columns=[\"_1\"])\n",
    "cross_corr_consistencies = cross_corr_quadrant[\n",
    "    cross_corr_quadrant[\"GPT_tone_name\"] == cross_corr_quadrant[\"human_tone_name\"]\n",
    "].sort_values(\"correlation\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_human_halves():\n",
    "    human_rating_responses = human_rating_trials[\"answer\"].dropna().apply(json.loads).apply(pd.Series)\n",
    "    human_ratings = human_rating_responses[\"answer\"].apply(pd.Series)\n",
    "    \n",
    "    testing_data = pd.merge(\n",
    "            human_rating_responses.drop(columns=[\"tones\", \"answer\"]),\n",
    "            human_ratings,\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\\\n",
    "            .melt(\"sentence\").dropna()\\\n",
    "            .groupby([\"sentence\", \"variable\"])\\\n",
    "            .apply(lambda s: s.sample(frac=1))\\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "    first_half_ind = testing_data.groupby([\"sentence\", \"variable\"])\\\n",
    "            .apply(lambda s: s.sample(frac=1/2))\\\n",
    "            .index.get_level_values(2)\n",
    "    \n",
    "    def get_pivot_version(df):\n",
    "        return_tbl = df.pivot_table(\n",
    "            index=\"sentence\",\n",
    "            columns=\"variable\",\n",
    "            values=\"value\",\n",
    "            aggfunc=lambda x: np.mean(x.dropna().astype(float))\n",
    "        )\n",
    "        return_tbl = return_tbl.loc[gpt_ratings_full.index.str.replace(\"'\", \"`\"), :]\n",
    "        return return_tbl\n",
    "    \n",
    "    return get_pivot_version(testing_data.loc[first_half_ind]),\\\n",
    "        get_pivot_version(testing_data.drop(first_half_ind))\n",
    "\n",
    "def make_gpt_halves():\n",
    "    first_half_ind = gpt_rating_trials.groupby([\"sentence\", \"tone\"])\\\n",
    "            .apply(lambda s: s.sample(frac=1/2))\\\n",
    "            .index.get_level_values(2)\n",
    "    \n",
    "    def get_pivot_version(df):\n",
    "        return_tbl = df.pivot_table(\n",
    "            index=\"sentence\",\n",
    "            columns=\"tone\",\n",
    "            values=\"current_rating\"\n",
    "        )\n",
    "        return_tbl.index = return_tbl.index.str.replace(\"'\", \"`\")\n",
    "        return return_tbl\n",
    "    \n",
    "    return get_pivot_version(gpt_rating_trials.loc[first_half_ind]),\\\n",
    "        get_pivot_version(gpt_rating_trials.drop(first_half_ind))\n",
    "\n",
    "def cross_corr_attenuate_bootstrap():\n",
    "    human_half_a, human_half_b = make_human_halves()\n",
    "    gpt_half_a, gpt_half_b = make_gpt_halves()\n",
    "    cross_corr_a = np.corrcoef(\n",
    "        human_half_a.T.iloc[human_sorting_label],\n",
    "        human_half_b.T.iloc[human_sorting_label]\n",
    "    )[:40, 40:]\n",
    "    cross_corr_b = np.corrcoef(\n",
    "        gpt_half_a.T.iloc[gpt_sorting_label],\n",
    "        gpt_half_b.T.iloc[gpt_sorting_label]\n",
    "    )[:40, 40:]\n",
    "    return np.diagonal(cross_corr_a), np.diagonal(cross_corr_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_cc_bootstrap(random_state):\n",
    "    cross_corr_matrix = pd.concat(\n",
    "        [\n",
    "            human_ratings_full_copy.sample(frac=0.9, random_state=random_state).T.iloc[human_sorting_label],\n",
    "            gpt_ratings_full_copy.sample(frac=0.9, random_state=random_state).T.iloc[gpt_sorting_label]\n",
    "        ]\n",
    "    ).T.corr()\n",
    "    cross_corr_quad = cross_corr_matrix.iloc[:40, 40:]\n",
    "    if np.isnan(cross_corr_quad.values).any():\n",
    "        return one_cc_bootstrap(random_state)\n",
    "    return np.diagonal(cross_corr_quad.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal_splithalf_a = []\n",
    "diagonal_splithalf_b = []\n",
    "for s in range(100):\n",
    "    curr_half_corr = cross_corr_attenuate_bootstrap()\n",
    "    if not np.isnan(curr_half_corr[0]).any():\n",
    "        diagonal_splithalf_a.append(curr_half_corr[0])\n",
    "    if not np.isnan(curr_half_corr[1]).any():\n",
    "        diagonal_splithalf_b.append(curr_half_corr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_diagonal_vals = np.vstack([one_cc_bootstrap(random_num) for random_num in range(5000)])\n",
    "diagonal_means_a = np.vstack(diagonal_splithalf_a).mean(axis=0)\n",
    "diagonal_means_b = np.vstack(diagonal_splithalf_b).mean(axis=0)\n",
    "attenuated_corr = np.mean(cc_diagonal_vals, axis=0) / np.sqrt(diagonal_means_a * diagonal_means_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_plot = pd.Series(attenuated_corr, index=human_ratings_full.columns[human_sorting_label])\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"index\": \"tone\", 0: \"value\"})\n",
    "cross_corr_consistencies_to_plot = cross_corr_consistencies[[\"GPT_tone_name\", \"correlation\"]]\\\n",
    "    .rename(columns={\"GPT_tone_name\": \"tone\", \"correlation\": \"value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bargraph_data = pd.DataFrame(\n",
    "    data = {\n",
    "        \"mean\": np.mean(cc_diagonal_vals, axis=0),\n",
    "        \"attenuated_mean\": attenuated_corr,\n",
    "        \"lower_ci\": np.percentile(cc_diagonal_vals, 2.5, axis=0),\n",
    "        \"high_ci\": np.percentile(cc_diagonal_vals, 97.5, axis=0),\n",
    "        \"tone_name\": cross_corr_quadrant[\n",
    "            cross_corr_quadrant[\"GPT_tone_name\"] == cross_corr_quadrant[\"human_tone_name\"]\n",
    "        ][\"human_tone_name\"]\n",
    "    }\n",
    ").sort_values(\"mean\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 15))\n",
    "sns.barplot(\n",
    "    data=bargraph_data,\n",
    "    y=\"tone_name\",\n",
    "    x=\"mean\",\n",
    "    orientation=\"horizontal\"\n",
    ")\n",
    "\n",
    "x_coords = [p.get_width() for p in ax.patches]\n",
    "y_coords = [p.get_y() + 0.5* p.get_height() for p in ax.patches]\n",
    "for x, y, tone_info in zip(\n",
    "    x_coords,\n",
    "    y_coords,\n",
    "    bargraph_data[\"tone_name\"]\n",
    "):\n",
    "    target_row = bargraph_data.query(f\"tone_name=='{tone_info}'\")\n",
    "    plt.errorbar(\n",
    "        x, y,\n",
    "        xerr=(target_row.iloc[:, 0] - target_row.iloc[:, 1], target_row.iloc[:, 2] - target_row.iloc[:, 0]),\n",
    "        fmt=\"none\",\n",
    "        c= \"k\",\n",
    "        capsize=5\n",
    "    )\n",
    "    \n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(-0.6, 39.6)\n",
    "\n",
    "plt.title(\"Conversation Tones Cross Correlation Matrix \\nDiagonal Terms\")\n",
    "plt.ylabel(\"Conversation Tone Name\")\n",
    "\n",
    "plt.savefig('figures/3c-errorbar.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Judgment Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing Similarity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_sjt = pd.read_csv(\n",
    "    \"../data/similarity_judgments/human_sjts.csv\"\n",
    ")[[\"answer\", \"word_1\", \"word_2\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_matrix_human(original_df, corr_index):\n",
    "    sims = original_df.pivot_table(\n",
    "        index=\"word_1\",\n",
    "        columns=\"word_2\",\n",
    "        values=\"answer\"\n",
    "    ).iloc[corr_index, corr_index].fillna(0)\n",
    "    vals = sims.values + sims.T.values\n",
    "    for i in range(40):\n",
    "        vals[i, i] /= 2\n",
    "    return vals\n",
    "\n",
    "human_on_tone_sim = get_sim_matrix_human(human_sjt, human_sorting_label) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halfsplit_correlation(df, sim_triu, sorting_label):\n",
    "    df_shuffled_corr = None\n",
    "    while df_shuffled_corr is None or np.isnan(df_shuffled_corr).any():\n",
    "        df_shuffled_corr = df.sample(frac=1.0, replace=True).corr().iloc[sorting_label, sorting_label].values[np.triu_indices(40, 1)]\n",
    "    return np.corrcoef(df_shuffled_corr, sim_triu)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_matrix_gpt(original_df, corr_index):\n",
    "    sims = original_df.pivot_table(\n",
    "        index=\"tone_a\",\n",
    "        columns=\"tone_b\",\n",
    "        values=\"current_rating\"\n",
    "    ).iloc[corr_index, corr_index].fillna(0)\n",
    "    vals = sims.values + sims.T.values\n",
    "    for i in range(40):\n",
    "        vals[i, i] /= 2\n",
    "    return vals\n",
    "gpt_sjt = pd.read_csv(\"../data/similarity_judgments/gpt_sjts.csv\", sep=\"|\")\n",
    "gpt_on_tone_sim = get_sim_matrix_gpt(\n",
    "    gpt_sjt,\n",
    "    gpt_sorting_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_bootstrap_sim(sjt_df, sorting_label, grouping_cols, mat_method):\n",
    "    return mat_method(\n",
    "        sjt_df.groupby(grouping_cols)\\\n",
    "            .apply(lambda s: s.sample(frac=1, replace=True))\\\n",
    "            .reset_index(drop=True),\n",
    "        sorting_label\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Similarity Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(gpt_on_tone_sim,\n",
    "    vmin=0, vmax=1,\n",
    "    cmap=\"coolwarm\")\n",
    "ax.set_xticklabels(cross_corr_matrix.index[40:])\n",
    "ax.set_yticklabels(cross_corr_matrix.index[40:])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.title(\"GPT Conversation Tone Similarity Judgment Matrix\", fontsize=16)\n",
    "ax.set_xticklabels([s._text.split(\": \")[1] for s in ax.get_xticklabels()], fontsize=12)\n",
    "ax.set_yticklabels([s._text.split(\": \")[1] for s in ax.get_yticklabels()], fontsize=12)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.savefig('figures/3d-gpt.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 15))\n",
    "sns.heatmap(human_on_tone_sim,\n",
    "    vmin=0, vmax=1,\n",
    "    cmap=\"coolwarm\")\n",
    "ax.set_xticklabels(cross_corr_matrix.index[:40])\n",
    "ax.set_yticklabels(cross_corr_matrix.index[:40])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.title(\"Human Conversation Tone Similarity Judgment Matrix\", fontsize=16)\n",
    "ax.set_xticklabels([s._text.split(\": \")[1] for s in ax.get_xticklabels()], fontsize=12)\n",
    "ax.set_yticklabels([s._text.split(\": \")[1] for s in ax.get_yticklabels()], fontsize=12)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.savefig('figures/3d-human.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability and Correlation Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_boot = [\n",
    "    get_halfsplit_correlation(\n",
    "        human_ratings_full, human_on_tone_sim[np.triu_indices(40, 1)], human_sorting_label\n",
    "    ) for _ in range(5000)\n",
    "]\n",
    "gpt_boot = [\n",
    "    get_halfsplit_correlation(\n",
    "        gpt_ratings_full, gpt_on_tone_sim[np.triu_indices(40, 1)], gpt_sorting_label\n",
    "    ) for _ in range(5000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_corr_human_sim_boot = [\n",
    "    get_halfsplit_correlation(\n",
    "        gpt_ratings_full, human_on_tone_sim[np.triu_indices(40, 1)], human_sorting_label\n",
    "    ) for _ in range(5000)\n",
    "]\n",
    "human_corr_gpt_sim_boot = [\n",
    "    get_halfsplit_correlation(\n",
    "        human_ratings_full, gpt_on_tone_sim[np.triu_indices(40, 1)], gpt_sorting_label\n",
    "    ) for _ in range(5000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halfsplit_human_sim_corr():\n",
    "    first_half_inds = human_sjt.groupby([\"word_1\", \"word_2\"])\\\n",
    "        .apply(lambda s: s.sample(2)).index\\\n",
    "        .get_level_values(2)\n",
    "    first_half_sjt = human_sjt.loc[first_half_inds]\n",
    "    second_half_sjt = human_sjt.drop(index=first_half_inds)\n",
    "    first_half_sim = get_sim_matrix_human(first_half_sjt, human_sorting_label)\n",
    "    second_half_sim = get_sim_matrix_human(second_half_sjt, human_sorting_label)\n",
    "    return np.corrcoef(\n",
    "        first_half_sim[np.triu_indices(40, 1)],\n",
    "        second_half_sim[np.triu_indices(40, 1)]\n",
    "    )[0, 1]\n",
    "    \n",
    "def get_halfsplit_gpt_sim_corr():\n",
    "    first_half_inds = gpt_sjt.groupby([\"tone_a\", \"tone_b\"])\\\n",
    "        .apply(lambda s: s.sample(2)).index\\\n",
    "        .get_level_values(2)\n",
    "    first_half_sjt = gpt_sjt.loc[first_half_inds]\n",
    "    second_half_sjt = gpt_sjt.drop(index=first_half_inds)\n",
    "    first_half_sim = get_sim_matrix_gpt(first_half_sjt, gpt_sorting_label)\n",
    "    second_half_sim = get_sim_matrix_gpt(second_half_sjt, gpt_sorting_label)\n",
    "    return np.corrcoef(\n",
    "        first_half_sim[np.triu_indices(40, 1)],\n",
    "        second_half_sim[np.triu_indices(40, 1)]\n",
    "    )[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(gpt_sim_halves), np.percentile(gpt_sim_halves, 2.5), np.percentile(gpt_sim_halves, 97.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_sim_halves = [\n",
    "#     get_halfsplit_human_sim_corr()\n",
    "#     for _ in range(5000)\n",
    "# ]\n",
    "gpt_sim_halves = [\n",
    "    get_halfsplit_gpt_sim_corr()\n",
    "    for _ in range(5000)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CC Alignment Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intergroup_distances = np.linalg.norm(\n",
    "    np.vstack(\n",
    "        [\n",
    "            same_tone_indices[\"MDS_x_x\"] - same_tone_indices[\"MDS_x_y\"],\n",
    "            same_tone_indices[\"MDS_y_x\"] - same_tone_indices[\"MDS_y_y\"],\n",
    "        ]\n",
    "    ),\n",
    "    axis=0\n",
    ")\n",
    "intergroup_distances_df = pd.DataFrame(\n",
    "    data={\n",
    "        \"tone\": same_tone_indices[\"tone\"],\n",
    "        \"distances\": intergroup_distances,\n",
    "        \"-corr\": -cross_corr_consistencies\\\n",
    "            .set_index(\"GPT_tone_name\")\\\n",
    "            .loc[same_tone_indices[\"tone\"]][\"correlation\"].values\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_cross_corr_embeddings = pd.DataFrame(\n",
    "    index=cross_corr_matrix.index,\n",
    "    data=[[np.array(coord)] for coord in cross_corr_matrix.values.tolist()]\n",
    ")\n",
    "root_cross_corr_embeddings[\"source\"] = pd.Series(root_cross_corr_embeddings.index).apply(lambda x: x.split(\": \")[0]).values\n",
    "root_cross_corr_embeddings[\"tone\"] = pd.Series(root_cross_corr_embeddings.index).apply(lambda x: x.split(\": \")[1]).values\n",
    "root_cross_corr_embeddings = root_cross_corr_embeddings.rename(columns={0:\"coordinates\"})\n",
    "root_cross_corr_embeddings = pd.merge(\n",
    "    root_cross_corr_embeddings.query(\"source == 'Human'\").drop(columns=\"source\"),\n",
    "    root_cross_corr_embeddings.query(\"source == 'GPT'\").drop(columns=\"source\"),\n",
    "    left_on=\"tone\",\n",
    "    right_on=\"tone\",\n",
    "    suffixes=(\"_human\", \"_gpt\")\n",
    ")\n",
    "root_cross_corr_embeddings[\"distances\"] = np.linalg.norm(\n",
    "    np.vstack(\n",
    "        root_cross_corr_embeddings[\"coordinates_gpt\"] - root_cross_corr_embeddings[\"coordinates_human\"]\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_cc_matrix(random_state):\n",
    "    cross_corr_matrix = pd.concat(\n",
    "        [\n",
    "            human_ratings_full_copy.sample(frac=0.9, random_state=random_state).T.iloc[human_sorting_label],\n",
    "            gpt_ratings_full_copy.sample(frac=0.9, random_state=random_state).T.iloc[gpt_sorting_label]\n",
    "        ]\n",
    "    ).T.corr()\n",
    "    if np.isnan(cross_corr_matrix.values).any():\n",
    "        return one_cc_bootstrap(random_state + 5000)\n",
    "    cross_corr_matrix_MDS = MDS(random_state=42).fit_transform(cross_corr_matrix.values)\n",
    "    indices = cross_corr_matrix.index.str.split(\": \")\n",
    "    MDS_with_source_tones = pd.DataFrame(cross_corr_matrix_MDS)\\\n",
    "        .assign(source=[elem[0] for elem in indices], tone=[elem[1] for elem in indices])\n",
    "    MDS_with_source_tones = MDS_with_source_tones.rename(columns={0: \"x\", 1: \"y\"})\n",
    "    same_tone_indices = pd.merge(\n",
    "        MDS_with_source_tones.query(\"source=='Human'\"),\n",
    "        MDS_with_source_tones.query(\"source=='GPT'\"),\n",
    "        left_on=\"tone\", right_on=\"tone\",\n",
    "        suffixes=(\"_human\", \"_gpt\")\n",
    "    )\n",
    "    return same_tone_indices.iloc[:, [3, 0, 1, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_series = []\n",
    "for seed in range(5000):\n",
    "    cur_random_MDS = get_random_cc_matrix(seed)\n",
    "    distance = ((cur_random_MDS.iloc[:,1] - cur_random_MDS.iloc[:,3]) ** 2 + (cur_random_MDS.iloc[:,2] - cur_random_MDS.iloc[:,4]) ** 2) ** 0.5\n",
    "    distance_series.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.DataFrame(distance_series)\n",
    "distances_df.columns = get_random_cc_matrix(0).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_bargraph_data = pd.DataFrame(\n",
    "    data = {\n",
    "        \"mean\": np.mean(distances_df, axis=0),\n",
    "        \"lower_ci\": np.percentile(distances_df, 2.5, axis=0),\n",
    "        \"higher_ci\": np.percentile(distances_df, 97.5, axis=0),\n",
    "        \"tone_name\": distances_df.columns\n",
    "    }\n",
    ").sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "sns.barplot(\n",
    "    data=dist_bargraph_data,\n",
    "    x=\"tone_name\",\n",
    "    y=\"mean\"\n",
    ")\n",
    "\n",
    "x_coords = [p.get_x() + 0.5*p.get_width() for p in ax.patches]\n",
    "y_coords = [p.get_height() for p in ax.patches]\n",
    "for x, y, tone_info in zip(\n",
    "    x_coords,\n",
    "    y_coords,\n",
    "    dist_bargraph_data[\"tone_name\"]\n",
    "):\n",
    "    target_row = dist_bargraph_data.query(f\"tone_name=='{tone_info}'\")\n",
    "    plt.errorbar(\n",
    "        x, y,\n",
    "        yerr=(target_row.iloc[:, 0] - target_row.iloc[:, 1], target_row.iloc[:, 2] - target_row.iloc[:, 0]),\n",
    "        fmt=\"none\",\n",
    "        c= \"k\",\n",
    "        capsize=5\n",
    "    )\n",
    "    \n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlim(-0.6, 39.6)\n",
    "\n",
    "plt.title(\"Conversation Tones Cross Correlation Alignment, Literal Pair Distances\")\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), fontsize=18)\n",
    "# ax.set_yticklabels(ax.get_yticklabels(), fontsize=18)\n",
    "plt.ylabel(\"Conversation Tone Name\")\n",
    "plt.savefig('figures/4b-distances.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 3))\n",
    "\n",
    "sns.barplot(\n",
    "    root_cross_corr_embeddings[[\"tone\", \"distances\"]].sort_values(\"distances\", ascending=False),\n",
    "    x=\"tone\",\n",
    "    y=\"distances\"\n",
    ")\n",
    "\n",
    "plt.title(\"Euclidean Distance of Conversation Tones in Cross-Correlational Shared Space\")\n",
    "plt.xlabel(\"Conversation Tone\")\n",
    "plt.ylabel(\"Euclidean Distances\")\n",
    "plt.xticks(rotation=80)\n",
    "\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 10))\n",
    "\n",
    "sns.barplot(\n",
    "    intergroup_distances_df[[\"tone\", \"distances\"]].sort_values(\"distances\", ascending=False),\n",
    "    y=\"tone\",\n",
    "    x=\"distances\",\n",
    "    orientation=\"horizontal\"\n",
    ")\n",
    "\n",
    "plt.title(\"Euclidean Distance of Conversation Tones \\n in Cross-Correlational Shared Space\")\n",
    "plt.ylabel(\"Conversation Tone\")\n",
    "plt.xlabel(\"Euclidean Distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_coords = np.vstack(np.vstack(root_cross_corr_embeddings[\"coordinates_human\"]))\n",
    "gpt_coords = np.vstack(np.vstack(root_cross_corr_embeddings[\"coordinates_gpt\"]))\n",
    "human_containing_nn = NearestNeighbors().fit(human_coords)\n",
    "gpt_containing_nn = NearestNeighbors().fit(gpt_coords)\n",
    "ind_to_words = lambda x: [root_cross_corr_embeddings[\"tone\"][ind] for ind in x]\n",
    "\n",
    "human_to_gpt_nn_map = {\n",
    "    ind: gpt_containing_nn.kneighbors(human_coords[ind].reshape((1,80)))\n",
    "    for ind in range(human_coords.shape[0])\n",
    "}\n",
    "gpt_to_human_nn_map = {\n",
    "    ind: human_containing_nn.kneighbors(gpt_coords[ind].reshape((1,80)))\n",
    "    for ind in range(gpt_coords.shape[0])\n",
    "}\n",
    "\n",
    "# human_to_gpt_nn_map = {\n",
    "#     root_cross_corr_embeddings[\"tone\"][ind]: ind_to_words(gpt_containing_nn.kneighbors(human_coords[ind].reshape((1,80)))[1])\n",
    "#     for ind in range(human_coords.shape[0])\n",
    "# }\n",
    "# gpt_to_human_nn_map = {\n",
    "#     root_cross_corr_embeddings[\"tone\"][ind]: ind_to_words(human_containing_nn.kneighbors(gpt_coords[ind].reshape((1,80)))[1])\n",
    "#     for ind in range(gpt_coords.shape[0])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "y_mult = 1.05\n",
    "y_axes = y_mult * np.arange(40)\n",
    "plt.scatter(2 * np.ones(40), y_axes, color=\"red\")\n",
    "plt.scatter(3 * np.ones(40), y_axes, color=\"blue\")\n",
    "plt.scatter(3.5 * np.ones(40), y_axes, color=\"blue\")\n",
    "plt.scatter(4.5 * np.ones(40), y_axes, color=\"red\")\n",
    "\n",
    "for human_tone in human_to_gpt_nn_map:\n",
    "    human_matched_target = human_to_gpt_nn_map[human_tone][1][0][0]\n",
    "    plt.plot(\n",
    "        [2, 3],\n",
    "        [human_tone * y_mult, human_matched_target * y_mult]\n",
    "    )\n",
    "    ax.annotate(root_cross_corr_embeddings[\"tone\"][human_tone], [1.9, human_tone * y_mult], ha=\"right\", va=\"center\")\n",
    "    ax.annotate(root_cross_corr_embeddings[\"tone\"][human_tone], [4.6, human_tone * y_mult], ha=\"left\", va=\"center\")\n",
    "\n",
    "for gpt_tone in gpt_to_human_nn_map:\n",
    "    gpt_matched_target = gpt_to_human_nn_map[gpt_tone][1][0][0]\n",
    "    plt.plot(\n",
    "        [3.5, 4.5],\n",
    "        [gpt_tone * y_mult, gpt_matched_target * y_mult]\n",
    "    )\n",
    "    ax.annotate(root_cross_corr_embeddings[\"tone\"][gpt_tone], [3.25, gpt_tone * y_mult], ha=\"center\", va=\"center\")\n",
    "plt.xlim((1.5, 5))\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_connection_bootstrap(n_iter=5000):\n",
    "    human_to_gpt_nn_map = {ind: [] for ind in range(40)}\n",
    "    gpt_to_human_nn_map = {ind: [] for ind in range(40)}\n",
    "    for s in range(n_iter):\n",
    "        test_cross_corr_matrix = pd.concat(\n",
    "            [\n",
    "                human_ratings_full_copy.sample(frac=0.9, random_state=s).T.iloc[human_sorting_label],\n",
    "                gpt_ratings_full_copy.sample(frac=0.9, random_state=s).T.iloc[gpt_sorting_label]\n",
    "            ]\n",
    "        ).T.corr()\n",
    "        root_cross_corr_embeddings = pd.DataFrame(\n",
    "            index=test_cross_corr_matrix.index,\n",
    "            data=[[np.array(coord)] for coord in test_cross_corr_matrix.values.tolist()]\n",
    "        )\n",
    "        root_cross_corr_embeddings[\"source\"] = pd.Series(root_cross_corr_embeddings.index).apply(lambda x: x.split(\": \")[0]).values\n",
    "        root_cross_corr_embeddings[\"tone\"] = pd.Series(root_cross_corr_embeddings.index).apply(lambda x: x.split(\": \")[1]).values\n",
    "        root_cross_corr_embeddings = root_cross_corr_embeddings.rename(columns={0:\"coordinates\"})\n",
    "        root_cross_corr_embeddings = pd.merge(\n",
    "            root_cross_corr_embeddings.query(\"source == 'Human'\").drop(columns=\"source\"),\n",
    "            root_cross_corr_embeddings.query(\"source == 'GPT'\").drop(columns=\"source\"),\n",
    "            left_on=\"tone\",\n",
    "            right_on=\"tone\",\n",
    "            suffixes=(\"_human\", \"_gpt\")\n",
    "        )\n",
    "        root_cross_corr_embeddings[\"distances\"] = np.linalg.norm(\n",
    "            np.vstack(\n",
    "                root_cross_corr_embeddings[\"coordinates_gpt\"] - root_cross_corr_embeddings[\"coordinates_human\"]\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        human_coords = np.vstack(np.vstack(root_cross_corr_embeddings[\"coordinates_human\"]))\n",
    "        gpt_coords = np.vstack(np.vstack(root_cross_corr_embeddings[\"coordinates_gpt\"]))\n",
    "        human_containing_nn = NearestNeighbors(n_neighbors=1).fit(human_coords)\n",
    "        gpt_containing_nn = NearestNeighbors(n_neighbors=1).fit(gpt_coords)\n",
    "\n",
    "        for ind in range(gpt_coords.shape[0]):\n",
    "            human_to_gpt_nn_map[ind].append(gpt_containing_nn.kneighbors(human_coords[ind].reshape((1,80)))[1][0][0])\n",
    "            gpt_to_human_nn_map[ind].append(human_containing_nn.kneighbors(gpt_coords[ind].reshape((1,80)))[1][0][0])\n",
    "    \n",
    "    for key in human_to_gpt_nn_map:\n",
    "        human_to_gpt_nn_map[key] = (\n",
    "            pd.Series(human_to_gpt_nn_map[key])\\\n",
    "                .value_counts() / n_iter\n",
    "        ).to_dict()\n",
    "    \n",
    "    \n",
    "    for key in gpt_to_human_nn_map:\n",
    "        gpt_to_human_nn_map[key] = (\n",
    "            pd.Series(gpt_to_human_nn_map[key])\\\n",
    "                .value_counts() / n_iter\n",
    "        ).to_dict()\n",
    "    \n",
    "    return human_to_gpt_nn_map, gpt_to_human_nn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_data = get_one_connection_bootstrap(n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "y_mult = 1.05\n",
    "y_axes = y_mult * np.arange(40)\n",
    "plt.scatter(2 * np.ones(40), y_axes, color=\"red\")\n",
    "plt.scatter(3 * np.ones(40), y_axes, color=\"blue\")\n",
    "plt.scatter(3.5 * np.ones(40), y_axes, color=\"blue\")\n",
    "plt.scatter(4.5 * np.ones(40), y_axes, color=\"red\")\n",
    "\n",
    "for human_tone in human_to_gpt_nn_map:\n",
    "    edge_set = connection_data[0][human_tone]\n",
    "    for edge_target, edge_weight in edge_set.items():\n",
    "        human_matched_target = edge_target\n",
    "        plt.plot(\n",
    "            [2, 3],\n",
    "            [human_tone * y_mult, human_matched_target * y_mult],\n",
    "            color=\"black\",\n",
    "            alpha=edge_weight\n",
    "        )\n",
    "    ax.annotate(root_cross_corr_embeddings[\"tone\"][human_tone], [1.9, human_tone * y_mult], ha=\"right\", va=\"center\")\n",
    "    ax.annotate(root_cross_corr_embeddings[\"tone\"][human_tone], [4.6, human_tone * y_mult], ha=\"left\", va=\"center\")\n",
    "\n",
    "for gpt_tone in gpt_to_human_nn_map:\n",
    "    edge_set = connection_data[1][gpt_tone]\n",
    "    for edge_target, edge_weight in edge_set.items():\n",
    "        gpt_matched_target = edge_target\n",
    "        plt.plot(\n",
    "            [3.5, 4.5],\n",
    "            [gpt_tone * y_mult, gpt_matched_target * y_mult],\n",
    "            color=\"black\",\n",
    "            alpha=edge_weight\n",
    "        )\n",
    "    ax.annotate(root_cross_corr_embeddings[\"tone\"][gpt_tone], [3.25, gpt_tone * y_mult], ha=\"center\", va=\"center\")\n",
    "plt.xlim((1.5, 5))\n",
    "ax.axis(\"off\")\n",
    "plt.savefig(\"./figures/4c.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alignment\n",
    "import external_ot\n",
    "# importlib.reload(external_ot)\n",
    "\n",
    "def simple_ot(human_ratings_full, gpt_ratings_full):\n",
    "    METRIC = \"cosine\"\n",
    "    p = ot.unif(40)\n",
    "    q = ot.unif(40)\n",
    "    # wp_human_ratings_full = get_resorted_table(human_ratings_full, same_tone_indices[\"human_index\"])\n",
    "    # wp_gpt_ratings_full = get_resorted_table(gpt_ratings_full, same_tone_indices[\"gpt_index\"])\n",
    "    wp_distances_human = cdist(human_ratings_full.to_numpy().T, human_ratings_full.to_numpy().T, metric=METRIC)\n",
    "    wp_distances_GPT = cdist(gpt_ratings_full.to_numpy().T, gpt_ratings_full.to_numpy().T, metric=METRIC)\n",
    "\n",
    "    gw, log = ot.gromov.entropic_gromov_wasserstein(\n",
    "        wp_distances_human, wp_distances_GPT, p, q, 'square_loss', epsilon=0.001, log=True\n",
    "    )\n",
    "    ot_human_MDS = MDS().fit_transform(human_ratings_full.T)\n",
    "    ot_gpt_MDS = MDS().fit_transform(gpt_ratings_full.T)\n",
    "    \n",
    "    BTA = ot_human_MDS.T @ (ot_gpt_MDS.T @ gw.T).T\n",
    "    svd_UVT = np.linalg.svd(BTA)\n",
    "    Q = svd_UVT[0] @ svd_UVT[2]\n",
    "    aligned_gpt_MDS = ot_gpt_MDS @ Q.T\n",
    "    return ot_human_MDS, aligned_gpt_MDS, 0\n",
    "\n",
    "def simple_procrustes(human_ratings_full, gpt_ratings_full):\n",
    "    ot_human_MDS = MDS().fit_transform(human_ratings_full.T)\n",
    "    ot_gpt_MDS = MDS().fit_transform(gpt_ratings_full.T)\n",
    "    BTA = ot_human_MDS.T @ ot_gpt_MDS\n",
    "    svd_UVT = np.linalg.svd(BTA)\n",
    "    Q = svd_UVT[0] @ svd_UVT[2]\n",
    "    aligned_gpt_MDS = ot_gpt_MDS @ Q.T\n",
    "    return ot_human_MDS, aligned_gpt_MDS, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_devices(embeddings_mat):\n",
    "    nn_devices = {\n",
    "        \"human_contained\": {},\n",
    "        \"gpt_contained\": {}\n",
    "    }\n",
    "    for k in range(1, 6):\n",
    "        nn_devices[\"human_contained\"][k] = NearestNeighbors(n_neighbors=k).fit(embeddings_mat[:40])\n",
    "        nn_devices[\"gpt_contained\"][k] = NearestNeighbors(n_neighbors=k).fit(embeddings_mat[40:])\n",
    "    return nn_devices\n",
    "\n",
    "cross_corr_nn_devices = get_nn_devices(cross_corr_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_corr_nn_cache = {}\n",
    "for tone in cross_corr_matrix.index:\n",
    "    tone_source, tone_name = tone.split(\": \")\n",
    "    ind_to_investigate = \"gpt_contained\" if tone_source == \"Human\" else \"human_contained\"\n",
    "    if tone_source not in cross_corr_nn_cache:\n",
    "        cross_corr_nn_cache[tone_source] = {}\n",
    "    cross_corr_nn_cache[tone_source][tone_name] = {}\n",
    "    for k in range(1, 6):\n",
    "        tone_kneighbors = cross_corr_nn_devices[ind_to_investigate][k].kneighbors(\n",
    "            cross_corr_matrix.loc[tone].values.reshape(1, -1)\n",
    "        )\n",
    "        cross_corr_nn_cache[tone_source][tone_name][k] = human_ratings_full.columns[human_sorting_label][tone_kneighbors[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ot_nn_info(human_emb_source, gpt_emb_source, alignment_func, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    human_MDS, gpt_MDS, _ = alignment_func(human_emb_source, gpt_emb_source)\n",
    "    stoc_ot_senemb_nns = get_nn_devices(np.concatenate([human_MDS, gpt_MDS]))\n",
    "    stoc_ot_senemb_nn_cache = {}\n",
    "    for tone_id, tone_name in enumerate(human_ratings_full.columns):\n",
    "        ind_to_investigate = \"gpt_contained\"\n",
    "        if \"Human\" not in stoc_ot_senemb_nn_cache:\n",
    "            stoc_ot_senemb_nn_cache[\"Human\"] = {}\n",
    "        stoc_ot_senemb_nn_cache[\"Human\"][tone_name] = {}\n",
    "        for k in range(1, 6):\n",
    "            tone_kneighbors = stoc_ot_senemb_nns[ind_to_investigate][k].kneighbors(\n",
    "                human_MDS[tone_id].reshape(1, -1)\n",
    "            )\n",
    "            stoc_ot_senemb_nn_cache[\"Human\"][tone_name][k] = human_ratings_full.columns[tone_kneighbors[1][0]]\n",
    "        \n",
    "        ind_to_investigate = \"human_contained\"\n",
    "        if \"GPT\" not in stoc_ot_senemb_nn_cache:\n",
    "            stoc_ot_senemb_nn_cache[\"GPT\"] = {}\n",
    "        stoc_ot_senemb_nn_cache[\"GPT\"][tone_name] = {}\n",
    "        for k in range(1, 6):\n",
    "            tone_kneighbors = stoc_ot_senemb_nns[ind_to_investigate][k].kneighbors(\n",
    "                human_MDS[tone_id].reshape(1, -1)\n",
    "            )\n",
    "            stoc_ot_senemb_nn_cache[\"GPT\"][tone_name][k] = gpt_ratings_full.columns[tone_kneighbors[1][0]]\n",
    "    return stoc_ot_senemb_nn_cache, {\n",
    "        \"human_MDS\": human_MDS,\n",
    "        \"gpt_MDS\": gpt_MDS,\n",
    "        \"human_tones\": human_emb_source.columns.to_list(),\n",
    "        \"gpt_tones\": gpt_emb_source.columns.to_list()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_inc_nn_subcache(source_ratings, target_ratings, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    curr_subcache = {\n",
    "        tone_name: {i: [] for i in range(1, 6)}\n",
    "        for tone_name in human_ratings_full.columns\n",
    "    }\n",
    "    nearest_neighbor = alignment.induce_one_side(\n",
    "        source_ratings=source_ratings,\n",
    "        target_ratings=target_ratings,\n",
    "        csls_neighborhood=5,\n",
    "        translation_csls_neighborhood=5,\n",
    "        direction=\"backward\",\n",
    "        n_induced_entries=1\n",
    "    )\n",
    "    for from_tone, to_tone in zip(nearest_neighbor[\"from\"], nearest_neighbor[\"to\"]):\n",
    "        curr_subcache[from_tone][1].append(to_tone)\n",
    "    for k in range(2, 6):\n",
    "        nn_data = alignment.induce_one_side(\n",
    "            source_ratings=source_ratings,\n",
    "            target_ratings=target_ratings,\n",
    "            csls_neighborhood=5,\n",
    "            translation_csls_neighborhood=5,\n",
    "            direction=\"backward\",\n",
    "            n_induced_entries=k\n",
    "        )\n",
    "        for from_tone, to_tone in nn_data.items():\n",
    "            curr_subcache[from_tone][k] = (to_tone)\n",
    "    return curr_subcache\n",
    "\n",
    "def get_knn_matchings(cache_1, cache_2):\n",
    "    matches = {}\n",
    "    total_investigations = {}\n",
    "    for tone_source in cache_1:\n",
    "        for tone in cache_1[tone_source]:\n",
    "            for neighbor_k in cache_1[tone_source][tone]:\n",
    "                if neighbor_k not in matches:\n",
    "                    matches[neighbor_k] = 0\n",
    "                    total_investigations[neighbor_k] = 0\n",
    "                total_investigations[neighbor_k] += neighbor_k\n",
    "                matches[neighbor_k] += np.in1d(\n",
    "                    cache_1[tone_source][tone][neighbor_k],\n",
    "                    cache_2[tone_source][tone][neighbor_k]\n",
    "                ).sum()\n",
    "    return matches, total_investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Alignment Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_data_on_NN(seed):\n",
    "    stoc_ot_senemb_nn_cache, stoc_ot_senemb_mds = get_ot_nn_info(human_ratings_full, gpt_ratings_full, external_ot.master_ot, seed)\n",
    "    simple_proc_senemb_nn_cache, simple_proc_senemb_mds = get_ot_nn_info(human_ratings_full, gpt_ratings_full, simple_procrustes, seed)\n",
    "    lexicon_induction_sememb_nn_cache = {\n",
    "        \"Human\": lex_inc_nn_subcache(human_ratings_full, gpt_ratings_full, seed),\n",
    "        \"GPT\": lex_inc_nn_subcache(gpt_ratings_full, human_ratings_full, seed)\n",
    "    }\n",
    "    nn_caches = {\n",
    "        \"stoc_ot_senemb_nn_cache\": stoc_ot_senemb_nn_cache,\n",
    "        \"simple_proc_senemb_nn_cache\": simple_proc_senemb_nn_cache,\n",
    "        \"lexicon_induction_sememb_nn_cache\": lexicon_induction_sememb_nn_cache,\n",
    "    }\n",
    "    match_rates = {}\n",
    "    for cache_key, cache_dict in nn_caches.items():\n",
    "        matchings = get_knn_matchings(cross_corr_nn_cache, cache_dict)\n",
    "        match_rates[cache_key] = [matchings[0][i] / matchings[1][i] for i in range(1, 6)]\n",
    "\n",
    "    match_rates_df = pd.DataFrame(\n",
    "        data = match_rates\n",
    "    )\n",
    "    return match_rates_df, {\n",
    "        \"stoc_ot_senemb_mds\": stoc_ot_senemb_mds,\n",
    "        \"simple_proc_senemb_mds\": simple_proc_senemb_mds,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(100):\n",
    "    current_seed_matchings, current_seed_data = get_matching_data_on_NN(seed)\n",
    "    for source_name in current_seed_data:\n",
    "        pd.concat([\n",
    "            pd.DataFrame(\n",
    "                index=[f\"human: {x}\" for x in current_seed_data[source_name][\"human_tones\"]],\n",
    "                data=current_seed_data[source_name][\"human_MDS\"],\n",
    "            ),\n",
    "            pd.DataFrame(\n",
    "                index=[f\"gpt: {x}\" for x in current_seed_data[source_name][\"gpt_tones\"]],\n",
    "                data=current_seed_data[source_name][\"gpt_MDS\"],\n",
    "            )\n",
    "        ]).to_csv(f\"../data/alignment_benchmarking/normal/seed{seed}_{source_name}.csv\")\n",
    "    # current_seed_matchings.to_csv(f\"../data/alignment_benchmarking/normal/seed{seed}_macthing_rates.csv\")\n",
    "    del current_seed_matchings\n",
    "    del current_seed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_of_align = [\"stoc_ot_senemb_mds\", \"simple_proc_senemb_mds\", \"lexicon_induction_sememb_nn_cache\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather kNN matching rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_df = []\n",
    "for s in range(100):\n",
    "    mix_df.append(pd.read_csv(f\"../data/alignment_benchmarking/normal/seed{s}_macthing_rates.csv\").drop(columns=\"Unnamed: 0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather correlation of dissimilarity and distance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_human_sim = human_on_tone_sim[np.triu_indices(40, 1)]\n",
    "target_gpt_sim = gpt_on_tone_sim[np.triu_indices(40, 1)]\n",
    "human_data_bli = []\n",
    "gpt_data_bli = []\n",
    "tone_order = [s.split(\": \")[1] for s in cross_corr_matrix.index[:40]]\n",
    "\n",
    "while len(gpt_data_bli) < 5000:\n",
    "    try:\n",
    "        src_embeddings, tgt_embeddings = alignment.induce_one_side(\n",
    "            source_ratings=human_ratings_full,\n",
    "            target_ratings=gpt_ratings_full,\n",
    "            csls_neighborhood=5,\n",
    "            translation_csls_neighborhood=5,\n",
    "            direction=\"backward\",\n",
    "            return_embeddings=True\n",
    "        ).values()\n",
    "        human_dist = cdist(pd.DataFrame(src_embeddings)[tone_order].T, pd.DataFrame(src_embeddings)[tone_order].T)\n",
    "        human_data_bli.append(\n",
    "            np.corrcoef(target_human_sim, -human_dist[np.triu_indices(40, 1)])[0, 1]\n",
    "        )\n",
    "        # print(\"appended to human\")\n",
    "\n",
    "        gpt_dist = cdist(pd.DataFrame(tgt_embeddings)[tone_order].T, pd.DataFrame(tgt_embeddings)[tone_order].T)\n",
    "        gpt_data_bli.append(\n",
    "            np.corrcoef(target_gpt_sim, -gpt_dist[np.triu_indices(40, 1)])[0, 1]\n",
    "        )\n",
    "        # print(\"appended to gpt\")\n",
    "    except KeyError:\n",
    "        a = 0\n",
    "        # print(\"keyError passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_arr = {method_name: {\"human\": [], \"gpt\": []} for method_name in methods_of_align}\n",
    "target_ind_order = cross_corr_matrix.index.str.lower()\n",
    "for s in range(100):\n",
    "    for method_name in methods_of_align:\n",
    "        testing_data = pd.read_csv(f\"../data/alignment_benchmarking/normal/seed{s}_{method_name}.csv\")\n",
    "        testing_data[\"tone_split_info\"] = testing_data[\"Unnamed: 0\"].str.split(\": \")\n",
    "        testing_data[\"tone_source\"] = testing_data[\"tone_split_info\"].str[0]\n",
    "        testing_data[\"tone_name\"] = testing_data[\"tone_split_info\"].str[1]\n",
    "        testing_data = testing_data.set_index(\"Unnamed: 0\").loc[target_ind_order]\n",
    "        \n",
    "        \n",
    "        human_data = testing_data.query(\"tone_source=='human'\").loc[:, [\"0\", \"1\"]]\n",
    "        human_dist = cdist(human_data, human_data)\n",
    "        distance_arr[method_name][\"human\"].append(\n",
    "            np.corrcoef(target_human_sim, -human_dist[np.triu_indices(40, 1)])[0, 1]\n",
    "        )\n",
    "        \n",
    "        gpt_data = testing_data.query(\"tone_source=='gpt'\").loc[:, [\"0\", \"1\"]]\n",
    "        gpt_dist = cdist(gpt_data, gpt_data)\n",
    "        distance_arr[method_name][\"gpt\"].append(\n",
    "            np.corrcoef(target_gpt_sim, -gpt_dist[np.triu_indices(40, 1)])[0, 1]\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_name in methods_of_align:\n",
    "    curr_df = pd.DataFrame(data=distance_arr[method_name])\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        {method_name}\n",
    "        \n",
    "        HUMAN:\n",
    "        mean: {curr_df[\"human\"].mean()}\n",
    "        low_ci: {np.percentile(curr_df[\"human\"], 2.5)}\n",
    "        high_ci: {np.percentile(curr_df[\"human\"], 97.5)}\n",
    "        \n",
    "        GPT:\n",
    "        mean: {curr_df[\"gpt\"].mean()}\n",
    "        low_ci: {np.percentile(curr_df[\"gpt\"], 2.5)}\n",
    "        high_ci: {np.percentile(curr_df[\"gpt\"], 97.5)}\n",
    "        \"\"\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
