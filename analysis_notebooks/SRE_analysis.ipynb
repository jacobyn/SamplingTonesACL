{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRE Results Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gensim.downloader\n",
    "import random\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.spatial.distance import cosine, jensenshannon, euclidean\n",
    "from scipy.stats import entropy, gaussian_kde\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.patheffects as patheffects\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Data Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_trial_fifty_df = pd.read_csv(\"../data/human_en_fifty/data/SRETrial.csv\")\n",
    "human_node_fifty_df = pd.read_csv(\"../data/human_en_fifty/data/SRENode.csv\")\n",
    "human_trial_thirty_df = pd.read_csv(\"../data/human_en_thirty/data/SRETrial.csv\")\n",
    "human_node_thirty_df = pd.read_csv(\"../data/human_en_thirty/data/SRENode.csv\")\n",
    "human_trial_twenty_df = pd.read_csv(\"../data/human_en_twenty/data/SRETrial.csv\")\n",
    "human_node_twenty_df = pd.read_csv(\"../data/human_en_twenty/data/SRENode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compact_trial_df(human_trial_df, human_node_df):\n",
    "    human_node_degrees = human_node_df[[\"id\", \"degree\"]]\n",
    "    human_trial_df_compact = human_trial_df.query(\"failed==False\")[[\"id\", \"origin_id\", \"network_id\", \"previous_sample\", \"obtained_response\", \"time_taken\"]]\n",
    "    human_trial_df_compact = human_trial_df_compact\\\n",
    "        .rename(columns = {\"id\": \"trial_id\"})\\\n",
    "        .set_index(\"trial_id\")\n",
    "    human_trial_df_compact[\"previous_sample\"] = human_trial_df_compact[\"previous_sample\"].map(json.loads)\n",
    "    human_trial_df_compact[\"provided_prompt\"] = human_trial_df_compact[\"previous_sample\"].map(lambda x: x[\"obtained_response\"])\n",
    "    human_trial_df_compact[\"node_mode\"] = human_trial_df_compact[\"previous_sample\"].map(lambda x: x[\"current_mode\"])\n",
    "    human_trial_df_compact = human_trial_df_compact\\\n",
    "                        .drop(columns=[\"previous_sample\"])\\\n",
    "                        .iloc[:, [0, 1, 5, 4, 2, 3]].dropna()\n",
    "    human_trial_df_compact = human_trial_df_compact.merge(human_node_degrees, left_on=\"origin_id\", right_on=\"id\").drop(columns=\"id\")\n",
    "    return human_trial_df_compact.query(\"network_id > 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_between_old_new(old_compact, new_comapct, degree_incr):\n",
    "    old_new_network_id_map = new_comapct\\\n",
    "        .query(\"degree==0\")[[\"network_id\", \"provided_prompt\"]]\\\n",
    "        .merge(\n",
    "            old_compact.query(f\"degree=={degree_incr}\")[[\"network_id\", \"obtained_response\"]],\n",
    "            left_on=\"provided_prompt\",\n",
    "            right_on=\"obtained_response\"\n",
    "        )\\\n",
    "        .drop_duplicates(subset=[\"network_id_y\", \"obtained_response\"])[[\"network_id_x\", \"network_id_y\"]]\n",
    "    old_new_network_id_map = {pt[0]: pt[1] for pt in old_new_network_id_map.values}\n",
    "    return old_new_network_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_fifty_compact = get_compact_trial_df(human_trial_fifty_df, human_node_fifty_df)\n",
    "human_thirty_compact = get_compact_trial_df(human_trial_thirty_df, human_node_thirty_df)\n",
    "old_new_network_id_map = get_map_between_old_new(human_fifty_compact, human_thirty_compact, 49)\n",
    "human_thirty_compact[\"network_id\"] = human_thirty_compact[\"network_id\"].replace(to_replace=old_new_network_id_map)\n",
    "human_thirty_compact[\"degree\"] = human_thirty_compact[\"degree\"] + 50\n",
    "human_eighty_compact = pd.concat([human_fifty_compact, human_thirty_compact])\n",
    "human_twenty_compact = get_compact_trial_df(human_trial_twenty_df, human_node_twenty_df)\n",
    "old_new_network_id_map = get_map_between_old_new(human_eighty_compact, human_twenty_compact, 79)\n",
    "human_twenty_compact[\"network_id\"] = human_twenty_compact[\"network_id\"].replace(to_replace=old_new_network_id_map)\n",
    "human_twenty_compact[\"degree\"] = human_twenty_compact[\"degree\"] + 80\n",
    "human_trial_df_compact = pd.concat([human_eighty_compact, human_twenty_compact])\n",
    "human_practice_trials, human_chain_trials = human_trial_df_compact.query(\"network_id <= 2\"), human_trial_df_compact.query(\"network_id > 2\")\n",
    "human_chain_trials_only_tones_c = human_chain_trials.query(\"node_mode=='c'\").drop(columns=\"provided_prompt\").rename(columns={\"obtained_response\": \"withholding_tone\"})\n",
    "human_chain_trials_only_tones_s = human_chain_trials.query(\"node_mode=='s' and degree==0\").drop(columns=\"obtained_response\").rename(columns={\"provided_prompt\": \"withholding_tone\"})\n",
    "# In the above interpretation, tones from iteration involve only tones that are sampled at the particular iteration.\n",
    "human_chain_trials_only_tones = pd.concat([human_chain_trials_only_tones_c, human_chain_trials_only_tones_s])\n",
    "human_chain_trials_only_tones = human_chain_trials_only_tones.sort_values([\"network_id\", \"degree\"])\n",
    "human_chain_trials_only_tones[\"withholding_tone\"] = human_chain_trials_only_tones[\"withholding_tone\"].str.lower()\n",
    "human_wanted_words = human_chain_trials_only_tones[\"withholding_tone\"].value_counts().index\n",
    "human_chain_trials_only_tones = human_chain_trials_only_tones[human_chain_trials_only_tones[\"withholding_tone\"].isin(human_wanted_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_tone_occurrence_table_whole = pd.pivot_table(\n",
    "    human_chain_trials_only_tones,\n",
    "    index=\"withholding_tone\",\n",
    "    columns=\"degree\",\n",
    "    values=\"origin_id\",\n",
    "    aggfunc=\"count\"\n",
    ").fillna(0)\n",
    "human_tone_occurrence_table_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Data Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_df = pd.read_csv(\"../data/totalGPTData.csv\", sep=\"|\", engine='python')\n",
    "gpt_tones_df = gpt_df.query(\"node_mode == 'c'\")\n",
    "gpt_tones_df[\"node_response\"] = gpt_tones_df[\"node_response\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tone_occurrence_table_whole = pd.pivot_table(\n",
    "    gpt_tones_df.reset_index(),\n",
    "    index=\"node_response\",\n",
    "    columns=\"node_order\",\n",
    "    values=\"index\",\n",
    "    aggfunc=\"count\"\n",
    ").fillna(0).iloc[:, :100]\n",
    "gpt_tone_occurrence_table_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding Tones to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_count(tone_array, all_40_tones):\n",
    "    return pd.merge(\n",
    "        tone_array / np.sum(tone_array),\n",
    "        pd.DataFrame(data = [0 for _ in range(len(all_40_tones))], index=all_40_tones),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"right\",\n",
    "    ).fillna(0)[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_top_24_tones = human_chain_trials_only_tones[\"withholding_tone\"].value_counts()\n",
    "gpt_top_24_tones = gpt_tones_df[\"node_response\"].value_counts()\n",
    "human_top_24_tones = (human_top_24_tones / np.sum(human_top_24_tones)).iloc[:24]\n",
    "gpt_top_24_tones = (gpt_top_24_tones / np.sum(gpt_top_24_tones)).iloc[:24]\n",
    "all_40_tones = pd.Index(set(gpt_top_24_tones.index).union(human_top_24_tones.index))\n",
    "\n",
    "human_total_td_data = []\n",
    "gpt_total_td_data = []\n",
    "\n",
    "for _ in range(250):\n",
    "    human_tone_dist_data = human_chain_trials_only_tones\\\n",
    "        .sample(frac=1, replace=True)[\"withholding_tone\"]\\\n",
    "        .value_counts()\n",
    "    gpt_tone_dist_data = gpt_tones_df\\\n",
    "        .sample(frac=1, replace=True)[\"node_response\"]\\\n",
    "        .value_counts()\n",
    "    human_tone_dist_data = get_true_count(\n",
    "        human_tone_dist_data / human_tone_dist_data.sum(),\n",
    "        all_40_tones\n",
    "    )\n",
    "    gpt_tone_dist_data = get_true_count(\n",
    "        gpt_tone_dist_data / gpt_tone_dist_data.sum(),\n",
    "        all_40_tones\n",
    "    )\n",
    "    \n",
    "    human_total_td_data.append(human_tone_dist_data)\n",
    "    gpt_total_td_data.append(gpt_tone_dist_data)\n",
    "\n",
    "human_top_24_tones = pd.concat([human_top_24_tones] + human_total_td_data)\n",
    "gpt_top_24_tones = pd.concat([gpt_top_24_tones] + gpt_total_td_data)\n",
    "\n",
    "human_top_24_tones = pd.DataFrame(human_top_24_tones).reset_index()\n",
    "human_top_24_tones.index = human_top_24_tones[\"index\"].values\n",
    "gpt_top_24_tones = pd.DataFrame(gpt_top_24_tones).reset_index()\n",
    "gpt_top_24_tones.index = gpt_top_24_tones[\"index\"].values\n",
    "\n",
    "top_tones_merged = human_top_24_tones.merge(\n",
    "    gpt_top_24_tones,\n",
    "    how=\"outer\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    suffixes=(\"_human\", \"_gpt\")\n",
    ").fillna(0)\n",
    "top_tones_merged_for_ordering = top_tones_merged\\\n",
    "    .assign(total_popularity=top_tones_merged.loc[:,\"count_human\"] + top_tones_merged.loc[:,\"count_gpt\"])\\\n",
    "    .sort_values(\"total_popularity\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_corrs = []\n",
    "for hd, gd in zip(human_total_td_data, gpt_total_td_data):\n",
    "    histogram_corrs.append(\n",
    "        np.corrcoef(hd.sort_index(), gd.sort_index())[0, 1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organized_top_tones_merged = top_tones_merged[[\"index_human\", \"count_human\", \"count_gpt\"]]\n",
    "organized_top_tones_merged.columns = [\"index\", \"Human Conversation Tones\", \"GPT Conversation Tones\"]\n",
    "tone_overlay = organized_top_tones_merged.melt(\"index\").sort_values(\"value\", ascending=False)\n",
    "tone_overlay = tone_overlay.groupby([\"index\", \"variable\"]).agg(\n",
    "    [np.mean, lambda x:np.percentile(x, 2.5), lambda y:np.percentile(y, 97.5)]\n",
    ")\n",
    "tone_overlay.columns = tone_overlay.columns.droplevel(0)\n",
    "tone_overlay = tone_overlay.reset_index()\n",
    "tone_overlay.columns = [\"index\", \"variable\", \"mean\", \"lower_ci\", \"higher_ci\"]\n",
    "tone_overlay = tone_overlay.sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_order = tone_overlay.groupby(\"index\").agg({\"mean\": np.mean}).sort_values(\"mean\").index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting for Distribution of Conversation Tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 30))\n",
    "target_df = tone_overlay.query(\"variable=='Human Conversation Tones'\")\n",
    "ax = sns.barplot(\n",
    "    data=target_df,\n",
    "    y=\"index\",\n",
    "    x=\"mean\",\n",
    "    order=histogram_order,\n",
    "    color=\"orange\"\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.xlabel(\"Conversation Tones\", fontsize=20)\n",
    "plt.ylabel(\"Frequency of Tones\", fontsize=20)\n",
    "# plt.setp(ax.get_legend().get_texts(), fontsize='18')\n",
    "# plt.setp(ax.get_legend().get_title(), fontsize='20')\n",
    "\n",
    "x_coords = [p.get_width() for p in ax.patches]\n",
    "y_coords = [p.get_y() + 0.5 * p.get_height() for p in ax.patches]\n",
    "for x, y, tone_info in zip(\n",
    "    x_coords,\n",
    "    y_coords,\n",
    "    [\n",
    "        (tone_source, tone_name)\n",
    "        for tone_source in [\"Human Conversation Tones\"]\n",
    "        for tone_name in histogram_order #target_df[\"index\"]\n",
    "    ]\n",
    "):\n",
    "    target_row = tone_overlay.query(f\"index=='{tone_info[1]}' and variable=='{tone_info[0]}'\")\n",
    "    print(tone_info)\n",
    "    plt.errorbar(\n",
    "        x, y,\n",
    "        xerr=(target_row.iloc[:, -3] - target_row.iloc[:, -2], target_row.iloc[:, -1] - target_row.iloc[:, -3]),\n",
    "        fmt=\"none\",\n",
    "        c= \"k\",\n",
    "        capsize=5,\n",
    "        \n",
    "    )\n",
    "    \n",
    "plt.ylim(-0.8, 40)\n",
    "plt.xlim(0, 0.25)\n",
    "\n",
    "# ax.get_legend().set_title(\"Conversation Tone Source Coloring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 30))\n",
    "target_df = tone_overlay.query(\"variable=='GPT Conversation Tones'\")\n",
    "ax = sns.barplot(\n",
    "    data=target_df,\n",
    "    y=\"index\",\n",
    "    x=\"mean\",\n",
    "    order=histogram_order,\n",
    "    color=\"blue\"\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.xlabel(\"Conversation Tones\", fontsize=20)\n",
    "plt.ylabel(\"Frequency of Tones\", fontsize=20)\n",
    "# plt.setp(ax.get_legend().get_texts(), fontsize='18')\n",
    "# plt.setp(ax.get_legend().get_title(), fontsize='20')\n",
    "\n",
    "x_coords = [p.get_width() for p in ax.patches]\n",
    "y_coords = [p.get_y() + 0.5* p.get_height() for p in ax.patches]\n",
    "for x, y, tone_info in zip(\n",
    "    x_coords,\n",
    "    y_coords,\n",
    "    [\n",
    "        (tone_source, tone_name)\n",
    "        for tone_source in [\"GPT Conversation Tones\"]\n",
    "        for tone_name in histogram_order #target_df[\"index\"]\n",
    "    ]\n",
    "):\n",
    "    target_row = tone_overlay.query(f\"index=='{tone_info[1]}' and variable=='{tone_info[0]}'\")\n",
    "    print(tone_info)\n",
    "    plt.errorbar(\n",
    "        x, y,\n",
    "        xerr=(target_row.iloc[:, -3] - target_row.iloc[:, -2], target_row.iloc[:, -1] - target_row.iloc[:, -3]),\n",
    "        fmt=\"none\",\n",
    "        c= \"k\",\n",
    "        capsize=5\n",
    "    )\n",
    "    \n",
    "plt.ylim(-0.8, 40)\n",
    "\n",
    "# ax.get_legend().set_title(\"Conversation Tone Source Coloring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 30))\n",
    "ax = sns.barplot(\n",
    "    data=tone_overlay,\n",
    "    y=\"index\",\n",
    "    x=\"mean\",\n",
    "    hue=\"variable\",\n",
    "    hue_order=[\"Human Conversation Tones\", \"GPT Conversation Tones\"],\n",
    "    order=histogram_order,\n",
    "    palette=[\"C1\", \"C0\"]\n",
    ")\n",
    "plt.xticks(rotation=75)\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.xlabel(\"Conversation Tones\", fontsize=20)\n",
    "plt.ylabel(\"Frequency of Tones\", fontsize=20)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='18')\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='20')\n",
    "\n",
    "x_coords = [p.get_width() for p in ax.patches]\n",
    "y_coords = [p.get_y() + 0.5* p.get_height() for p in ax.patches]\n",
    "for x, y, tone_info in zip(\n",
    "    x_coords,\n",
    "    y_coords,\n",
    "    [\n",
    "        (tone_source, tone_name)\n",
    "        for tone_source in [\"Human Conversation Tones\", \"GPT Conversation Tones\"]\n",
    "        for tone_name in histogram_order\n",
    "    ]\n",
    "):\n",
    "    target_row = tone_overlay.query(f\"index=='{tone_info[1]}' and variable=='{tone_info[0]}'\")\n",
    "    plt.errorbar(\n",
    "        x, y,\n",
    "        xerr=(target_row.iloc[:, -3] - target_row.iloc[:, -2], target_row.iloc[:, -1] - target_row.iloc[:, -3]),\n",
    "        fmt=\"none\",\n",
    "        c= \"k\",\n",
    "        capsize=5\n",
    "    )\n",
    "    \n",
    "plt.ylim(-0.8, 40)\n",
    "\n",
    "ax.get_legend().set_title(\"Conversation Tone Source Coloring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability of Human, GPT Sample Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halfsplit_tone_distribution_correlation(tone_df, tone_column_label, all_tones_list, network_id_col=\"network_id\"):\n",
    "    shuffled_df = tone_df.sample(frac=1).reset_index(drop=True)\n",
    "    shuffled_sentence_inds = tone_df[network_id_col].unique()\n",
    "    np.random.shuffle(shuffled_sentence_inds)\n",
    "    sentence_inds_half_a, sentence_inds_half_b = shuffled_sentence_inds[:len(shuffled_sentence_inds) // 2], shuffled_sentence_inds[len(shuffled_sentence_inds) // 2:]\n",
    "    df_halfsplit_a, df_halfsplit_b = shuffled_df[shuffled_df[network_id_col].isin(sentence_inds_half_a)], shuffled_df[shuffled_df[network_id_col].isin(sentence_inds_half_b)]\n",
    "    def resulting_distribution(tone_series, all_tones_list):\n",
    "        distribution_dict = {}\n",
    "        distrbution_arr = np.array([])\n",
    "        for tone in all_tones_list:\n",
    "            tone_count = np.sum(tone_series == tone)\n",
    "            distribution_dict[tone] = tone_count\n",
    "            distrbution_arr = np.append(distrbution_arr, tone_count)\n",
    "        return distribution_dict, distrbution_arr\n",
    "\n",
    "    distribution_half_a = resulting_distribution(df_halfsplit_a[tone_column_label], all_tones_list)\n",
    "    distribution_half_b = resulting_distribution(df_halfsplit_b[tone_column_label], all_tones_list)\n",
    "    return np.corrcoef(\n",
    "        distribution_half_a[1], distribution_half_b[1]\n",
    "    )[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_human_tones = human_chain_trials_only_tones[\"withholding_tone\"].unique()\n",
    "human_dist_corrs = np.array([])\n",
    "np.random.seed(42)\n",
    "for _ in range(5000):\n",
    "    human_dist_corrs = np.append(\n",
    "        human_dist_corrs,\n",
    "        get_halfsplit_tone_distribution_correlation(\n",
    "            human_chain_trials_only_tones, \"withholding_tone\", all_human_tones\n",
    "        )\n",
    "    )\n",
    "plt.hist(human_dist_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gpt_tones = gpt_tones_df[\"node_response\"].unique()\n",
    "gpt_dist_corrs = np.array([])\n",
    "np.random.seed(42)\n",
    "for _ in range(5000):\n",
    "    gpt_dist_corrs = np.append(\n",
    "        gpt_dist_corrs,\n",
    "        get_halfsplit_tone_distribution_correlation(\n",
    "            gpt_tones_df, \"node_response\", all_gpt_tones, \"chain_id\"\n",
    "        )\n",
    "    )\n",
    "plt.hist(gpt_dist_corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Space UMAP Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sent2vec.vectorizer import Vectorizer\n",
    "from umap import UMAP\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "human_chain_trials_only_sentences = human_chain_trials.query(\"node_mode=='s'\")\n",
    "gpt_df_sentences = gpt_df.query(\"node_mode == 's'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get embeddings again. Otherwise, they are attached in the repository already.\n",
    "\n",
    "# temp_human_chain_trials_semb = human_chain_trials_only_sentences\n",
    "# temp_gpt_chain_trials_semb = gpt_df_sentences\n",
    "# merged_sentence = pd.concat(\n",
    "#     [\n",
    "#         temp_gpt_chain_trials_semb[[\"node_order\", \"chain_id\", \"node_response\"]]\\\n",
    "#             .rename(\n",
    "#             columns={\n",
    "#                 \"chain_id\": \"network_id\",\n",
    "#                 \"node_order\": \"degree\",\n",
    "#                 \"node_response\": \"obtained_response\"\n",
    "#             }\n",
    "#             ).assign(reponse_source=[\"gpt\" for _ in range(temp_gpt_chain_trials_semb.shape[0])]),\n",
    "#         temp_human_chain_trials_semb[[\"network_id\", \"degree\", \"obtained_response\"]]\\\n",
    "#             .assign(reponse_source=[\"human\" for _ in range(temp_human_chain_trials_semb.shape[0])])\n",
    "#     ]\n",
    "# )\n",
    "# merging_vectorizer = Vectorizer()\n",
    "# merging_vectorizer.run(merged_sentence[\"obtained_response\"].tolist())\n",
    "# merging_vectors = merging_vectorizer.vectors\n",
    "# merged_sembs = merged_sentence.assign(sentence_embeddings=merging_vectors)\n",
    "# merged_vectors_transformed = UMAP(random_state=42).fit_transform(merging_vectors)\n",
    "# merged_sembs = merged_sembs.assign(sentence_embeddings_umap=merged_vectors_transformed.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the previously computed embeddings\n",
    "\n",
    "merged_sembs = pd.read_csv(\"../data/all_chains_semb_by_bert.csv\")\n",
    "merged_sembs[\"sentence_embeddings_umap\"] = merged_sembs[\"sentence_embeddings_umap\"].apply(json.loads)\n",
    "merged_sembs[\"degree\"] = merged_sembs[\"degree\"].apply(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Sentence Space Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blocked_sentence_space(data_source):\n",
    "    testing_data = pd.merge(\n",
    "        merged_sembs[[\"reponse_source\", \"degree\"]],\n",
    "        pd.DataFrame(data=data_source[\"sentence_embeddings_umap\"].to_list(), columns=[\"x\", \"y\"]),\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "    )\n",
    "    x_interval = (testing_data[\"x\"].max() - testing_data[\"x\"].min()) / 50\n",
    "    y_interval = (testing_data[\"y\"].max() - testing_data[\"y\"].min()) / 50\n",
    "    testing_data[\"x_blocked\"] = (testing_data[\"x\"] - testing_data[\"x\"].min()) // x_interval\n",
    "    testing_data[\"y_blocked\"] = (testing_data[\"y\"] - testing_data[\"y\"].min()) // y_interval\n",
    "    return testing_data\n",
    "\n",
    "def get_entropy_vec(entropy_df):\n",
    "    entropy_df_tbl = entropy_df.pivot_table(\n",
    "        index=\"x_blocked\",\n",
    "        columns=\"y_blocked\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0\n",
    "    )\n",
    "    entropy_df_tbl = entropy_df_tbl.reindex(np.arange(50), axis=0, fill_value=0)\n",
    "    entropy_df_tbl = entropy_df_tbl.reindex(np.arange(50), axis=1, fill_value=0)\n",
    "    entropy_vec = entropy_df_tbl.values.flatten()\n",
    "    return entropy_vec / entropy_vec.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sembs_entropy = get_blocked_sentence_space(merged_sembs)\n",
    "human_entropies = [\n",
    "    entropy(get_entropy_vec(merged_sembs_entropy.query(\"reponse_source=='human'\").sample(frac=1.0, replace=True, random_state=s)))\n",
    "    for s in range(5000)\n",
    "]\n",
    "gpt_entropies = [\n",
    "    entropy(get_entropy_vec(merged_sembs_entropy.query(\"reponse_source=='gpt'\").sample(frac=1.0, replace=True, random_state=s)))\n",
    "    for s in range(5000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(\n",
    "    np.arange(101),\n",
    "    [\n",
    "        entropy(\n",
    "            get_entropy_vec(\n",
    "                merged_sembs_entropy.query(f\"reponse_source=='human' and degree=={deg}\")\n",
    "            )\n",
    "        )\n",
    "        for deg in np.arange(101)\n",
    "    ],\n",
    "    label=\"Human entropy (indivudal iteration)\"\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(101),\n",
    "    [\n",
    "        entropy(\n",
    "            get_entropy_vec(\n",
    "                merged_sembs_entropy.query(f\"reponse_source=='gpt' and degree=={deg}\")\n",
    "            )\n",
    "        )\n",
    "        for deg in np.arange(101)\n",
    "    ],\n",
    "    label=\"GPT entropy (indivudal iteration)\"\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(1, 100),\n",
    "    [\n",
    "        np.linalg.norm(\n",
    "            get_entropy_vec(\n",
    "                merged_sembs_entropy.query(f\"reponse_source=='human' and degree=={deg}\")\n",
    "            ) - get_entropy_vec(\n",
    "                merged_sembs_entropy.query(f\"reponse_source=='gpt' and degree=={deg}\")\n",
    "            )\n",
    "        )\n",
    "        for deg in np.arange(1, 100)\n",
    "    ],\n",
    "    label=\"difference between human and GPT distribution (indivudal iteration)\"\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(1, 101),\n",
    "    [\n",
    "        np.linalg.norm(\n",
    "            get_entropy_vec(\n",
    "                merged_sembs_entropy.query(f\"reponse_source=='human' and degree<={deg}\")\n",
    "            ) - get_entropy_vec(\n",
    "                merged_sembs_entropy.query(f\"reponse_source=='gpt' and degree<={deg}\")\n",
    "            )\n",
    "        )\n",
    "        for deg in np.arange(1, 101)\n",
    "    ],\n",
    "    label=\"difference between human and GPT distribution (cumulative throughout iteration)\"\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting of Sentence Joint Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sembs[\"degree_response_source\"] = pd.Series(zip(merged_sembs[\"degree\"], merged_sembs[\"reponse_source\"])).values\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "human_colormap = cm.Reds(np.linspace(0, 1, 1 + max(merged_sembs.query(\"reponse_source=='human'\")[\"degree\"].unique())))\n",
    "gpt_colormap = cm.Blues(np.linspace(0, 1, 1 + max(merged_sembs.query(\"reponse_source=='gpt'\")[\"degree\"].unique())))\n",
    "colormap_maps = {\n",
    "    \"human\": human_colormap, \"gpt\": gpt_colormap\n",
    "}\n",
    "colormap_corrs = {\n",
    "    \"human\": human_colormap[len(human_colormap) // 2], \"gpt\": gpt_colormap[len(gpt_colormap) // 2]\n",
    "}\n",
    "for elem in merged_sembs[\"degree_response_source\"].unique():\n",
    "    embeddings_to_work_with = np.vstack(\n",
    "        merged_sembs[merged_sembs[\"degree_response_source\"]==elem][\"sentence_embeddings_umap\"].to_list()\n",
    "    )\n",
    "    plt.scatter(\n",
    "        x=embeddings_to_work_with[:, 0],\n",
    "        y=embeddings_to_work_with[:, 1],\n",
    "        c = np.vstack([colormap_corrs[elem[1]] for _ in range(len(embeddings_to_work_with))]),\n",
    "        alpha=0.03\n",
    "        # c=np.vstack([colormap_maps[elem[1]][elem[0]] for _ in range(len(embeddings_to_work_with))])\n",
    "    )\n",
    "\n",
    "circle_centroids = [\n",
    "    (-7.5, 8.5),\n",
    "    # (-1.5, 0),\n",
    "    # (1, 0),\n",
    "    (-1, 0),\n",
    "    (1, -4.5),\n",
    "    (-5, -2),\n",
    "    (3.5, -0.5),\n",
    "    (-3, -5),\n",
    "    (10.5, -8),\n",
    "    (-2, -12)\n",
    "]\n",
    "circle_radii = [\n",
    "    3.5,\n",
    "    # 1.3,\n",
    "    # 1.3,\n",
    "    2.5,\n",
    "    2,\n",
    "    1.5,\n",
    "    1.75,\n",
    "    2,\n",
    "    3.5,\n",
    "    2\n",
    "    \n",
    "]\n",
    "for centroid, radius in zip(circle_centroids, circle_radii):\n",
    "    ax.add_patch(\n",
    "        plt.Circle(centroid, radius, fill=False)\n",
    "    )\n",
    "plt.xlim((-24, 24))\n",
    "plt.ylim((-24, 24))\n",
    "\n",
    "plt.legend(\n",
    "    [\n",
    "        mpatches.Circle((0.0, 0.0), 0.05, facecolor=colormap_corrs[\"human\"]),\n",
    "        mpatches.Circle((0.0, 0.0), 0.05, facecolor=colormap_corrs[\"gpt\"])\n",
    "    ],\n",
    "    [\"Human Sentence Embeddings\", \"GPT Sentence Embeddings\"],\n",
    "    prop={'size': 18}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generations_to_look_at = [\n",
    "    0, 1, 3, 5, 15, 30, 40, 50, 60, 70, 80, 90, 100\n",
    "]\n",
    "for i in range(len(generations_to_look_at) - 1):\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    for elem in [(j, source) for j in range(generations_to_look_at[i], generations_to_look_at[i+1] + 1) for source in [\"gpt\", \"human\"]]:#merged_sembs[\"degree_response_source\"].unique():\n",
    "        if elem not in list(merged_sembs[\"degree_response_source\"].unique()):\n",
    "            print(elem)\n",
    "            continue\n",
    "        embeddings_to_work_with = np.vstack(\n",
    "            merged_sembs[merged_sembs[\"degree_response_source\"]==elem][\"sentence_embeddings_umap\"].to_list()\n",
    "        )\n",
    "        ax.scatter(\n",
    "            x=embeddings_to_work_with[:, 0],\n",
    "            y=embeddings_to_work_with[:, 1],\n",
    "            c = np.vstack([colormap_corrs[elem[1]] for _ in range(len(embeddings_to_work_with))]),\n",
    "            alpha=0.5\n",
    "        )\n",
    "        ax.set_title(f\"Distribution at Iteration {generations_to_look_at[i+1]}\", fontsize=35)\n",
    "    for centroid, radius in zip(circle_centroids, circle_radii):\n",
    "        ax.add_patch(\n",
    "            plt.Circle(centroid, radius, fill=False)\n",
    "        )\n",
    "    ax.set_xlim(xmin=-24, xmax=24)\n",
    "    ax.set_ylim(ymin=-24, ymax=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar Analysis on Tone Embedding Space instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_human_chain_trials_temb = human_chain_trials_only_tones\n",
    "temp_gpt_chain_trials_temb = gpt_tones_df\n",
    "merged_tones = pd.concat(\n",
    "    [\n",
    "        temp_gpt_chain_trials_temb[[\"node_order\", \"chain_id\", \"node_response\"]]\\\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"chain_id\": \"network_id\",\n",
    "                    \"node_order\": \"degree\",\n",
    "                    \"node_response\": \"obtained_response\"\n",
    "                }\n",
    "            ).assign(reponse_source=[\"gpt\" for _ in range(temp_gpt_chain_trials_temb.shape[0])]),\n",
    "        temp_human_chain_trials_temb[[\"network_id\", \"degree\", \"withholding_tone\"]]\\\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"withholding_tone\": \"obtained_response\"\n",
    "                }\n",
    "            )\\\n",
    "            .assign(reponse_source=[\"human\" for _ in range(temp_human_chain_trials_temb.shape[0])])\n",
    "    ]\n",
    ")\n",
    "merging_vectorizer_temb = Vectorizer()\n",
    "merging_vectorizer_temb.run(merged_tones[\"obtained_response\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merging_vectors_temb = merging_vectorizer_temb.vectors\n",
    "merged_tembs = merged_tones.assign(sentence_embeddings=merging_vectors_temb)\n",
    "merged_vectors_transformed = UMAP(random_state=42).fit_transform(merging_vectors_temb)\n",
    "merged_tembs = merged_tembs.assign(sentence_embeddings_umap=merged_vectors_transformed.tolist())\n",
    "merged_tembs[\"degree_response_source\"] = pd.Series(zip(merged_tembs[\"degree\"], merged_tembs[\"reponse_source\"])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "human_colormap = cm.Reds(np.linspace(0, 1, 1 + max(merged_tembs.query(\"reponse_source=='human'\")[\"degree\"].unique())))\n",
    "gpt_colormap = cm.Blues(np.linspace(0, 1, 1 + max(merged_tembs.query(\"reponse_source=='gpt'\")[\"degree\"].unique())))\n",
    "colormap_maps = {\n",
    "    \"human\": human_colormap, \"gpt\": gpt_colormap\n",
    "}\n",
    "colormap_corrs = {\n",
    "    \"human\": human_colormap[len(human_colormap) // 2], \"gpt\": gpt_colormap[len(gpt_colormap) // 2]\n",
    "}\n",
    "for elem in merged_tembs[\"degree_response_source\"].unique():\n",
    "    embeddings_to_work_with = np.vstack(\n",
    "        merged_tembs[merged_tembs[\"degree_response_source\"]==elem][\"sentence_embeddings_umap\"].to_list()\n",
    "    )\n",
    "    plt.scatter(\n",
    "        x=embeddings_to_work_with[:, 0],\n",
    "        y=embeddings_to_work_with[:, 1],\n",
    "        c = np.vstack([colormap_corrs[elem[1]] for _ in range(len(embeddings_to_work_with))]),\n",
    "        alpha=0.03\n",
    "        # c=np.vstack([colormap_maps[elem[1]][elem[0]] for _ in range(len(embeddings_to_work_with))])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generations_to_look_at = [\n",
    "    0, 1, 3, 5, 15, 30, 40, 50, 60, 70, 80, 90, 100\n",
    "]\n",
    "for i in range(len(generations_to_look_at) - 1):\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    for elem in [(j, source) for j in range(generations_to_look_at[i], generations_to_look_at[i+1] + 1) for source in [\"gpt\", \"human\"]]:#merged_sembs[\"degree_response_source\"].unique():\n",
    "        if elem not in list(merged_tembs[\"degree_response_source\"].unique()):\n",
    "            print(elem)\n",
    "            continue\n",
    "        embeddings_to_work_with = np.vstack(\n",
    "            merged_tembs[merged_tembs[\"degree_response_source\"]==elem][\"sentence_embeddings_umap\"].to_list()\n",
    "        )\n",
    "        ax.scatter(\n",
    "            x=embeddings_to_work_with[:, 0],\n",
    "            y=embeddings_to_work_with[:, 1],\n",
    "            c = np.vstack([colormap_corrs[elem[1]] for _ in range(len(embeddings_to_work_with))]),\n",
    "            alpha=0.7\n",
    "        )\n",
    "        ax.set_title(f\"Distribution at Iteration {generations_to_look_at[i+1]}\", fontsize=30)\n",
    "    # for centroid, radius in zip(circle_centroids, circle_radii):\n",
    "    #     ax.add_patch(\n",
    "    #         plt.Circle(centroid, radius, fill=False)\n",
    "    #     )\n",
    "    ax.set_xlim(xmin=-34, xmax=34)\n",
    "    ax.set_ylim(ymin=-34, ymax=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporation of Wordclouds into Sentence Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customizing wordcloud settings before use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_stopwords = wordcloud.STOPWORDS\n",
    "other_stopwords = [\n",
    "    \"really\", \"m\", \"much\", \"will\", \"s\", \"\", \"oh\", \"john\", \"david\"\n",
    "]\n",
    "for other_stopword in other_stopwords:\n",
    "    extended_stopwords.add(other_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_counter(target_partition, response_source):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    if response_source in [\"human\", \"gpt\"]:\n",
    "        target_partition = target_partition.query(f\"reponse_source=='{response_source}'\")\n",
    "    target_data = target_partition[\"obtained_response_parsed\"]\n",
    "    \n",
    "    if len(target_data) == 0: return None\n",
    "    \n",
    "    vectorizer = vectorizer.fit(target_data)\n",
    "    vectorized_data = vectorizer.transform([\" \".join(target_data)]).toarray()[0] #.max(axis=1)\n",
    "    \n",
    "    vectorized_data = vectorized_data / vectorized_data.max()\n",
    "    vectorized_features = vectorizer.get_feature_names_out()\n",
    "    all_words_tfidf = {\n",
    "        word: tfidf for word, tfidf in zip(vectorized_features, vectorized_data)\n",
    "    }\n",
    "    for word in extended_stopwords:\n",
    "        if word in all_words_tfidf:\n",
    "            del all_words_tfidf[word]\n",
    "    return all_words_tfidf\n",
    "\n",
    "def get_wordcloud_color(\n",
    "    human_freqs, gpt_freqs,\n",
    "):\n",
    "    def subfunction(word, font_size=None, position=None, orientation=None, random_state=None, font_path=None):\n",
    "        def helper(word, freqs):\n",
    "            if freqs is None or word not in freqs:\n",
    "                return 0\n",
    "            else:\n",
    "                return freqs[word]\n",
    "        ratio_to_color = lambda x: int(40 + x * 200)\n",
    "        human_freq = ratio_to_color(helper(word, human_freqs))\n",
    "        gpt_freq = ratio_to_color(helper(word, gpt_freqs))\n",
    "        return f\"rgb({human_freq}, 0, {gpt_freq})\"\n",
    "    return subfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the wordclouds at cusotm centroids, radii:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordcloud_at_cluster(target_centroid, epsilon):\n",
    "    merged_sembs_radius = merged_sembs.assign(\n",
    "        radius_from_centroid=merged_sembs[\"sentence_embeddings_umap\"].apply(\n",
    "            lambda x: np.linalg.norm(np.array(x) - target_centroid, ord=2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    target_partition = merged_sembs_radius[merged_sembs_radius[\"radius_from_centroid\"] <= epsilon]\n",
    "    \n",
    "    if len(target_partition) == 0:\n",
    "        return None\n",
    "    # print(merged_sembs_radius)\n",
    "    target_partition[\"obtained_response_parsed\"] = target_partition[\"obtained_response\"]\\\n",
    "            .str.replace(\"[,.!?:;]\", \" \")\\\n",
    "            .str.replace(r\"\\s+\", \" \")\\\n",
    "            .str.replace(r\"`\", \"'\")\\\n",
    "            .str.replace(r'\"', \"\")\\\n",
    "            .str.lower()\n",
    "    comment_words = \"\".join(target_partition[\"obtained_response_parsed\"].to_list())\n",
    "    \n",
    "    generated_wordcloud = WordCloud(\n",
    "        background_color ='white',\n",
    "        min_font_size = 10,\n",
    "        collocations = False,\n",
    "        max_words=80,\n",
    "        prefer_horizontal=1\n",
    "    )\n",
    "    filtered_words = generated_wordcloud.process_text(comment_words)\n",
    "    \n",
    "    def sample_and_regen(sentence, filtered_words):\n",
    "        return \" \".join([word for word in sentence.split(\" \") if word in filtered_words])\n",
    "    \n",
    "    target_partition[\"obtained_response_parsed\"] = target_partition[\"obtained_response_parsed\"]\\\n",
    "        .apply(lambda x: sample_and_regen(x, filtered_words))\n",
    "    \n",
    "    human_freqs = get_all_words_counter(target_partition, \"human\")\n",
    "    gpt_freqs = get_all_words_counter(target_partition, \"gpt\")\n",
    "    all_freqs = get_all_words_counter(target_partition, \"all\")\n",
    "    print(all_freqs)\n",
    "        \n",
    "    generated_wordcloud.color_func = get_wordcloud_color(human_freqs, gpt_freqs)\n",
    "    \n",
    "    generated_wordcloud.generate_from_frequencies(\n",
    "        {word: val ** 3 for word, val in all_freqs.items()}\n",
    "    )\n",
    "    # generated_wordcloud.generate(comment_words)\n",
    "    \n",
    "    \n",
    " \n",
    "    # # plot the WordCloud image                       \n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(generated_wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return generated_wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for centroid, radii in zip(circle_centroids, circle_radii):\n",
    "    print(centroid, radii)\n",
    "    get_wordcloud_at_cluster(centroid, radii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dims = 12\n",
    "fig, ax = plt.subplots(fig_dims, fig_dims, figsize=(100, 100))\n",
    "fig_radius = 48 // fig_dims / 2\n",
    "for row in range(fig_dims):\n",
    "    for col in range(fig_dims):\n",
    "        get_centroid_x = lambda x: (-24 + fig_radius) + x * (48 // fig_dims)\n",
    "        get_centroid_y = lambda x: -get_centroid_x(x)\n",
    "        centroid = (get_centroid_x(col), get_centroid_y(row))\n",
    "        generated_wordcloud = get_wordcloud_at_cluster(centroid, fig_radius)\n",
    "        # print(centroid, fig_radius)\n",
    "        if generated_wordcloud is not None:\n",
    "            ax[row, col].imshow(generated_wordcloud)\n",
    "        ax[row, col].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dims = 16\n",
    "fig, ax = plt.subplots(fig_dims, fig_dims, figsize=(100, 100))\n",
    "fig_radius = 48 // fig_dims / 2\n",
    "for row in range(fig_dims):\n",
    "    for col in range(fig_dims):\n",
    "        get_centroid_x = lambda x: (-24 + fig_radius) + x * (48 // fig_dims)\n",
    "        get_centroid_y = lambda x: -get_centroid_x(x)\n",
    "        centroid = (get_centroid_x(col), get_centroid_y(row))\n",
    "        generated_wordcloud = get_wordcloud_at_cluster(centroid, fig_radius)\n",
    "        # print(centroid, fig_radius)\n",
    "        if generated_wordcloud is not None:\n",
    "            ax[row, col].imshow(generated_wordcloud)\n",
    "        ax[row, col].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dims = 24\n",
    "fig, ax = plt.subplots(fig_dims, fig_dims, figsize=(100, 100))\n",
    "fig_radius = 48 // fig_dims / 2\n",
    "for row in range(fig_dims):\n",
    "    for col in range(fig_dims):\n",
    "        get_centroid_x = lambda x: (-24 + fig_radius) + x * (48 // fig_dims)\n",
    "        get_centroid_y = lambda x: -get_centroid_x(x)\n",
    "        centroid = (get_centroid_x(col), get_centroid_y(row))\n",
    "        generated_wordcloud = get_wordcloud_at_cluster(centroid, fig_radius)\n",
    "        # print(centroid, fig_radius)\n",
    "        if generated_wordcloud is not None:\n",
    "            ax[row, col].imshow(generated_wordcloud)\n",
    "        ax[row, col].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment on Sample Independnece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_sample_at_chain(chain_id, human_trial_df, human_tones_arr):\n",
    "    return pd.merge(\n",
    "        left=human_trial_df.query(f\"network_id=={chain_id}\"),\n",
    "        right=pd.DataFrame(human_tones_arr).rename(columns={0: \"tone\"}),\n",
    "        left_on=\"withholding_tone\",\n",
    "        right_on=\"tone\",\n",
    "        how=\"outer\"\n",
    "    ).reset_index().fillna(0).pivot_table(\n",
    "        values=\"index\",\n",
    "        aggfunc=\"count\",\n",
    "        index=\"degree\",\n",
    "        columns=\"tone\"\n",
    "    ).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_chain_trial_samples = {\n",
    "    network_id: get_human_sample_at_chain(network_id, human_chain_trials_only_tones, all_human_tones)\n",
    "    for network_id in range(3, 93)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_at_chain_gpt(chain_trial_df, all_tones_arr, chain_id):\n",
    "    return pd.merge(\n",
    "        left=chain_trial_df.query(f\"chain_id=={chain_id}\").reset_index(),\n",
    "        right=pd.DataFrame(all_tones_arr).rename(columns={0: \"tone\"}),\n",
    "        left_on=\"node_response\",\n",
    "        right_on=\"tone\",\n",
    "        how=\"outer\"\n",
    "    ).reset_index().fillna(0).pivot_table(\n",
    "        values=\"index\",\n",
    "        aggfunc=\"count\",\n",
    "        index=\"node_order\",\n",
    "        columns=\"tone\"\n",
    "    ).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_chain_trial_samples = {\n",
    "    network_id: get_table_at_chain_gpt(gpt_tones_df, all_gpt_tones, network_id)\n",
    "    for network_id in range(91)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blockify_df(df_to_block, time_block_width_fn):\n",
    "    group_allocation_column = time_block_width_fn(df_to_block.index)\n",
    "    df_to_block = df_to_block.assign(group_allocation=group_allocation_column)\n",
    "    return df_to_block.groupby(\"group_allocation\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_chain_trial_samples_blocked = {\n",
    "    df_key: blockify_df(df_to_block, lambda x: x)\n",
    "    for df_key, df_to_block in human_chain_trial_samples.items()\n",
    "}\n",
    "gpt_chain_trial_samples_blocked = {\n",
    "    df_key: blockify_df(df_to_block, lambda x: x)\n",
    "    for df_key, df_to_block in gpt_chain_trial_samples.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Measuremnt: $C_{j,t}, C_{j, {t+1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances_within_chain = []\n",
    "for n in range(3, 93):\n",
    "    test_matrix = human_chain_trial_samples_blocked[n].values\n",
    "    for i in range(len(test_matrix) - 1):\n",
    "        for j in range(i + 2, len(test_matrix) - 1):\n",
    "            all_distances_within_chain.append(np.corrcoef(test_matrix[i], test_matrix[j])[0, 1])\n",
    "plt.hist(all_distances_within_chain, bins=60)\n",
    "np.percentile(all_distances_within_chain, 2.5), np.percentile(all_distances_within_chain, 97.5), np.mean(all_distances_within_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances_within_chain = []\n",
    "for n in range(0, 91):\n",
    "    if n == 81: continue\n",
    "    test_matrix = gpt_chain_trial_samples_blocked[n].values\n",
    "    for i in range(len(test_matrix) - 1):\n",
    "        for j in range(i + 2, len(test_matrix) - 1):\n",
    "            corr_val = np.corrcoef(test_matrix[i], test_matrix[j])[0, 1]\n",
    "            all_distances_within_chain.append(corr_val)\n",
    "\n",
    "all_distances_within_chain = np.array(all_distances_within_chain)[~np.isnan(all_distances_within_chain)]\n",
    "plt.hist(all_distances_within_chain, bins=60)\n",
    "np.percentile(all_distances_within_chain, 2.5), np.percentile(all_distances_within_chain, 97.5), np.mean(all_distances_within_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances_within_chain = []\n",
    "all_distances_within_chain_across_i = {\n",
    "    \"means\": [],\n",
    "    \"pr2.5\": [],\n",
    "    \"pr97.5\": [],\n",
    "    \"median\": []\n",
    "}\n",
    "\n",
    "for i in range(100):\n",
    "    for n in range(0, 91):\n",
    "        if n == 81 or i not in gpt_chain_trial_samples_blocked[n].index:\n",
    "            continue\n",
    "        test_matrix = gpt_chain_trial_samples_blocked[n].values\n",
    "        for j in range(i + 2, len(test_matrix) - 1):\n",
    "            corr_val = np.corrcoef(test_matrix[i], test_matrix[j])[0, 1]\n",
    "            if not np.isnan(corr_val):\n",
    "                all_distances_within_chain.append(corr_val)\n",
    "    all_distances_within_chain_across_i[\"means\"].append(np.mean(all_distances_within_chain))\n",
    "    all_distances_within_chain_across_i[\"pr2.5\"].append(np.percentile(all_distances_within_chain, 2.5))\n",
    "    all_distances_within_chain_across_i[\"pr97.5\"].append(np.percentile(all_distances_within_chain, 97.5))\n",
    "    all_distances_within_chain_across_i[\"median\"].append(np.median(all_distances_within_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    np.arange(len(all_distances_within_chain_across_i[\"means\"])),\n",
    "    all_distances_within_chain_across_i[\"means\"], label=\"means\"\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(len(all_distances_within_chain_across_i[\"means\"])),\n",
    "    all_distances_within_chain_across_i[\"median\"], label=\"median\"\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(len(all_distances_within_chain_across_i[\"means\"])),\n",
    "    all_distances_within_chain_across_i[\"pr2.5\"], label=\"pr2.5\"\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(len(all_distances_within_chain_across_i[\"means\"])),\n",
    "    all_distances_within_chain_across_i[\"pr97.5\"], label=\"pr97.5\"\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Measurement: $C_{i,t}, C_{j,t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances_within_block = []\n",
    "for i in range(20):\n",
    "    for k in range(3, 92):\n",
    "        test_matrix_a = human_chain_trial_samples_blocked[k].values\n",
    "        if len(test_matrix_a) <= i:\n",
    "            continue\n",
    "        for n in range(k + 1, 93):\n",
    "            test_matrix_b = human_chain_trial_samples_blocked[n].values\n",
    "            if len(test_matrix_b) <= i:\n",
    "                continue\n",
    "            all_distances_within_block.append(np.corrcoef(test_matrix_a[i], test_matrix_b[i])[0, 1])\n",
    "plt.hist(all_distances_within_block, bins=20)\n",
    "(np.percentile(all_distances_within_block, 2.5), np.percentile(all_distances_within_block, 97.5), np.mean(all_distances_within_block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances_within_block = []\n",
    "for i in range(20):\n",
    "    for k in range(0, 89):\n",
    "        if k == 81: continue\n",
    "        test_matrix_a = gpt_chain_trial_samples_blocked[k].values\n",
    "        if len(test_matrix_a) <= i:\n",
    "            continue\n",
    "        for n in range(k + 1, 90):\n",
    "            if n == 81: continue\n",
    "            test_matrix_b = gpt_chain_trial_samples_blocked[n].values\n",
    "            if len(test_matrix_b) <= i:\n",
    "                continue\n",
    "            corr_value = np.corrcoef(test_matrix_a[i], test_matrix_b[i])[0, 1]\n",
    "            all_distances_within_block.append(corr_value)\n",
    "all_distances_within_block = np.array(all_distances_within_block)[~np.isnan(all_distances_within_block)]\n",
    "plt.hist(all_distances_within_block, bins=20)\n",
    "(np.percentile(all_distances_within_block, 2.5), np.percentile(all_distances_within_block, 97.5), np.mean(all_distances_within_block))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
